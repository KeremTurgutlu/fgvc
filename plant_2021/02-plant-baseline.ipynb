{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from self_supervised.layers import *\n",
    "from self_supervised.multimodal.clip import *\n",
    "import clip\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path(\"plant-pathology-2021-fgvc8/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#5) [Path('plant-pathology-2021-fgvc8/train.csv'),Path('plant-pathology-2021-fgvc8/test_images'),Path('plant-pathology-2021-fgvc8/train_images_512'),Path('plant-pathology-2021-fgvc8/train_images'),Path('plant-pathology-2021-fgvc8/sample_submission.csv')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(datapath/'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dist = train_df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scab                               4826\n",
       "healthy                            4624\n",
       "frog_eye_leaf_spot                 3181\n",
       "cider_apple_rust                   1860\n",
       "complex                            1602\n",
       "powdery_mildew                     1184\n",
       "scab frog_eye_leaf_spot             686\n",
       "scab frog_eye_leaf_spot complex     200\n",
       "frog_eye_leaf_spot complex          165\n",
       "rust frog_eye_leaf_spot             120\n",
       "rust complex                         97\n",
       "powdery_mildew complex               87\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['num_disease'] = train_df['labels'].apply(lambda o: len(o.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scab                  4826\n",
       "healthy               4624\n",
       "frog_eye_leaf_spot    3181\n",
       "cider_apple_rust      1860\n",
       "complex               1602\n",
       "powdery_mildew        1184\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.query(\"num_disease == 1\")['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scab frog_eye_leaf_spot            686\n",
       "scab frog_eye_leaf_spot complex    200\n",
       "frog_eye_leaf_spot complex         165\n",
       "rust frog_eye_leaf_spot            120\n",
       "rust complex                        97\n",
       "powdery_mildew complex              87\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.query(\"num_disease > 1\")['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_img(im): return PILImage.create(datapath/f'train_images/{im}')\n",
    "\n",
    "# img = read_img(train_df['image'][0])\n",
    "\n",
    "# img.shape\n",
    "\n",
    "# def resize_img(img, sz=512):\n",
    "#     targ_sz = resize_to(img, use_min=True, targ_sz=sz)\n",
    "#     return img.resize(targ_sz, resample=Image.BILINEAR).convert(\"RGB\")\n",
    "\n",
    "# resizepath = datapath/'train_images_512'\n",
    "\n",
    "# resizepath.mkdir()\n",
    "\n",
    "# fns = train_df['image'].values; fns[:5]\n",
    "\n",
    "# def read_resize_save(fn):\n",
    "#     img = read_img(fn)\n",
    "#     img = resize_img(img)\n",
    "#     img.save(resizepath/f'{fn}')\n",
    "\n",
    "# # parallel(read_resize_save, fns, progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = train_df['image'].values\n",
    "fn2label = dict(zip(train_df['image'], train_df['labels'].apply(lambda o: o.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(im): return PILImage.create(datapath/f'train_images_512/{im}')\n",
    "def read_label(fn): return fn2label[fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(files, bs=32, size=448, stats=imagenet_stats):\n",
    "\n",
    "    tfms = [[read_img, ToTensor, RandomResizedCrop(size, min_scale=.7)], \n",
    "            [read_label, MultiCategorize(), OneHotEncode]]\n",
    "\n",
    "    dsets = Datasets(files, tfms=tfms, splits=RandomSplitter(valid_pct=0.1, seed=0.2)(files))\n",
    "\n",
    "    batch_augs = aug_transforms()\n",
    "\n",
    "    batch_tfms = [IntToFloatTensor] + batch_augs + [Normalize.from_stats(*stats)]\n",
    "    dls = dsets.dataloaders(bs=bs, after_batch=batch_tfms)\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from fastai.callback.wandb import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_efficientnet_b4_ns - lr 2e-2, size 448, bs 32\n",
    "# resnet101d - lr 2e-2, size 448, bs 32\n",
    "# vit_base_patch16_384 - lr 1e-3 size 384, bs 32 (checkpoint 2)\n",
    "# vit_base_resnet50_384 - lr 1e-3 size 384 bs 24 (checkpoint 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export \n",
    "# from torch.utils.checkpoint import checkpoint_sequential\n",
    "    \n",
    "# class CheckpointVisionTransformer(Module):\n",
    "#     def __init__(self, vit_model, checkpoint_nchunks=2):\n",
    "#         self.checkpoint_nchunks = checkpoint_nchunks\n",
    "#         self.vit_model = vit_model\n",
    "    \n",
    "#     def forward_features(self, x):\n",
    "#         B = x.shape[0]\n",
    "#         x = self.vit_model.patch_embed(x)\n",
    "\n",
    "#         cls_tokens = self.vit_model.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "#         x = x + self.vit_model.pos_embed\n",
    "#         x = self.vit_model.pos_drop(x)\n",
    "#         x = checkpoint_sequential(self.vit_model.blocks, self.checkpoint_nchunks, x)\n",
    "#         x = self.vit_model.norm(x)[:, 0]\n",
    "#         x = self.vit_model.pre_logits(x)\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.forward_features(x)\n",
    "#         x = self.vit_model.head(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def name(self:F1ScoreMulti): return \"f1_multi_macro\"\n",
    "f1macro = F1ScoreMulti(average='macro')\n",
    "@patch\n",
    "def name(self:F1ScoreMulti): return \"f1_multi_micro\"\n",
    "f1micro = F1ScoreMulti(average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Tuple, Union\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = None\n",
    "        self.stride = stride\n",
    "\n",
    "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
    "            self.downsample = nn.Sequential(OrderedDict([\n",
    "                (\"-1\", nn.AvgPool2d(stride)),\n",
    "                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n",
    "                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.avgpool(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionPool2d(nn.Module):\n",
    "    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n",
    "        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n",
    "        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n",
    "        x, _ = F.multi_head_attention_forward(\n",
    "            query=x, key=x, value=x,\n",
    "            embed_dim_to_check=x.shape[-1],\n",
    "            num_heads=self.num_heads,\n",
    "            q_proj_weight=self.q_proj.weight,\n",
    "            k_proj_weight=self.k_proj.weight,\n",
    "            v_proj_weight=self.v_proj.weight,\n",
    "            in_proj_weight=None,\n",
    "            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n",
    "            bias_k=None,\n",
    "            bias_v=None,\n",
    "            add_zero_attn=False,\n",
    "            dropout_p=0,\n",
    "            out_proj_weight=self.c_proj.weight,\n",
    "            out_proj_bias=self.c_proj.bias,\n",
    "            use_separate_proj_weight=True,\n",
    "            training=self.training,\n",
    "            need_weights=False\n",
    "        )\n",
    "\n",
    "        return x[0]\n",
    "\n",
    "\n",
    "class ModifiedResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet class that is similar to torchvision's but contains the following changes:\n",
    "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
    "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
    "    - The final pooling layer is a QKV attention instead of an average pool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        # the 3-layer stem\n",
    "        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(width)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # residual layers\n",
    "        self._inplanes = width  # this is a *mutable* variable used during construction\n",
    "        self.layer1 = self._make_layer(width, layers[0])\n",
    "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n",
    "\n",
    "        embed_dim = width * 32  # the ResNet feature dimension\n",
    "        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "\n",
    "        self._inplanes = planes * Bottleneck.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self._inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        def stem(x):\n",
    "            for conv, bn in [(self.conv1, self.bn1), (self.conv2, self.bn2), (self.conv3, self.bn3)]:\n",
    "                x = self.relu(bn(conv(x)))\n",
    "            x = self.avgpool(x)\n",
    "            return x\n",
    "\n",
    "        x = x.type(self.conv1.weight.dtype)\n",
    "        x = stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.attnpool(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "            (\"gelu\", QuickGELU()),\n",
    "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "    def attention(self, x: torch.Tensor):\n",
    "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
    "        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None, checkpoint=False, checkpoint_nchunks=2):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
    "        self.checkpoint = checkpoint\n",
    "        self.checkpoint_nchunks = checkpoint_nchunks\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.checkpoint: return torch.utils.checkpoint.checkpoint_sequential(self.resblocks, self.checkpoint_nchunks, x)\n",
    "        else:               return self.resblocks(x)\n",
    "\n",
    "\n",
    "# class VisualTransformer(nn.Module):\n",
    "#     def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int, **kwargs):\n",
    "#         super().__init__()\n",
    "#         self.input_resolution = input_resolution\n",
    "#         self.output_dim = output_dim\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n",
    "\n",
    "#         scale = width ** -0.5\n",
    "#         self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "#         self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n",
    "#         self.ln_pre = LayerNorm(width)\n",
    "\n",
    "#         self.transformer = Transformer(width, layers, heads, **kwargs)\n",
    "\n",
    "#         self.ln_post = LayerNorm(width)\n",
    "#         self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
    "\n",
    "#     def forward(self, x: torch.Tensor):\n",
    "#         x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
    "#         x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "#         x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "#         x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "#         x = x + self.positional_embedding.to(x.dtype)\n",
    "#         x = self.ln_pre(x)\n",
    "\n",
    "#         x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "#         x = self.transformer(x)\n",
    "#         x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "#         x = self.ln_post(x[:, 0, :])\n",
    "\n",
    "#         if self.proj is not None:\n",
    "#             x = x @ self.proj\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "class VisualTransformer(nn.Module):\n",
    "    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int, patch_stride=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "        if patch_stride is None: patch_stride = patch_size\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_stride, bias=False)\n",
    "\n",
    "        scale = width ** -0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        \n",
    "        \n",
    "        num_pathces = (input_resolution - patch_size + patch_stride) // patch_stride\n",
    "        \n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn(num_pathces ** 2 + 1, width))\n",
    "        self.ln_pre = LayerNorm(width)\n",
    "\n",
    "        self.transformer = Transformer(width, layers, heads, **kwargs)\n",
    "\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "        x = self.ln_pre(x)\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "        x = self.ln_post(x[:, 0, :])\n",
    "\n",
    "        if self.proj is not None:\n",
    "            x = x @ self.proj\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 # vision\n",
    "                 image_resolution: int,\n",
    "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "                 vision_width: int,\n",
    "                 vision_patch_size: int,\n",
    "                 # text\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        if isinstance(vision_layers, (tuple, list)):\n",
    "            vision_heads = vision_width * 32 // 64\n",
    "            self.visual = ModifiedResNet(\n",
    "                layers=vision_layers,\n",
    "                output_dim=embed_dim,\n",
    "                heads=vision_heads,\n",
    "                input_resolution=image_resolution,\n",
    "                width=vision_width\n",
    "            )\n",
    "        else:\n",
    "            vision_heads = vision_width // 64\n",
    "            self.visual = VisualTransformer(\n",
    "                input_resolution=image_resolution,\n",
    "                patch_size=vision_patch_size,\n",
    "                width=vision_width,\n",
    "                layers=vision_layers,\n",
    "                heads=vision_heads,\n",
    "                output_dim=embed_dim,\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask=self.build_attention_mask(),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
    "        self.ln_final = LayerNorm(transformer_width)\n",
    "\n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1/0.07))) # Same initialization as paper\n",
    "\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "        \n",
    "        # visual model\n",
    "        if isinstance(self.visual, ModifiedResNet):\n",
    "            if self.visual.attnpool is not None:\n",
    "                std = self.visual.attnpool.c_proj.in_features ** -0.5\n",
    "                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n",
    "\n",
    "            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n",
    "                for name, param in resnet_block.named_parameters():\n",
    "                    if name.endswith(\"bn3.weight\"):\n",
    "                        nn.init.zeros_(param)\n",
    "        \n",
    "        # text model\n",
    "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
    "        attn_std = self.transformer.width ** -0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
    "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
    "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
    "\n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
    "\n",
    "    def build_attention_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(self.context_length, self.context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        return mask\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.conv1.weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(text)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return image_features, text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model..\n"
     ]
    }
   ],
   "source": [
    "do_finetune = True\n",
    "size = 448\n",
    "patch_size = 32\n",
    "patch_stride = 32\n",
    "\n",
    "if do_finetune:\n",
    "    if patch_size != 32: raise Exception(f\"Patch size needs to be 32 for pretrained model\")\n",
    "    print(\"Loading pretrained model..\")\n",
    "    vitb32_config_dict = vitb32_config(224, context_length=77, vocab_size=49408)\n",
    "    clip_model = CLIP(**vitb32_config_dict)\n",
    "    clip_pretrained_model, _ = clip.load(\"ViT-B/32\", jit=False)\n",
    "    clip_model.load_state_dict(clip_pretrained_model.state_dict())\n",
    "    \n",
    "    clip_vitb = clip_model.visual\n",
    "    num_patches = (size//patch_size)**2 +1\n",
    "    # interpolate positional embedding to match any input size\n",
    "    embed_dim = clip_vitb.positional_embedding.size(1)\n",
    "    clip_vitb.positional_embedding.data = F.interpolate(clip_vitb.positional_embedding.data[None, None, ...], \n",
    "                                                        size=[num_patches, embed_dim], \n",
    "                                                        mode='bilinear',align_corners=False)[0,0]\n",
    "    del clip_model, clip_pretrained_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    clip_vitb = VisualTransformer(size,patch_size=patch_size,width=768,layers=12,heads=12,output_dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,448,448).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_vitb = VisualTransformerSliding(size,patch_size=32,patch_stride=32,width=768,layers=12,heads=12,output_dim=512).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = clip_vitb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2727,  0.8291,  0.8253,  ..., -0.1570, -0.5043, -0.5108],\n",
       "        [-0.3180,  0.6770,  0.9188,  ..., -0.0938, -0.4522, -0.3193]],\n",
       "       device='cuda:0', grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(448 - 32 + 32) / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = clip_vitb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1194, -0.0074,  0.2404,  ...,  0.2923, -0.2274,  0.0151],\n",
       "        [-0.1485, -0.0169,  0.3211,  ...,  0.2932, -0.1752, -0.0494]],\n",
       "       device='cuda:0', grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = \"vit_base_resnet50_384\"\n",
    "bs, size, lr, epochs = 24, 384, 2e-2, 30\n",
    "dls = get_dls(files[:5000],bs,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WANDB:\n",
    "    xtra_config = {\"Arch\":arch, \"Size\":size, \"lr\": lr}\n",
    "    wandb.init(project=\"plant-pathology-2021\", config=xtra_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "encoder = create_encoder(arch, pretrained=False, n_in=3)\n",
    "if arch == 'vit_base_patch16_384':  encoder = CheckpointVisionTransformer(encoder, 2)\n",
    "if arch == 'vit_base_resnet50_384': encoder = CheckpointVisionTransformer(encoder, 12)\n",
    "# if arch == 'tf_efficientnet_b4_ns': encoder = CheckpointEfficientNet(encoder, 2)\n",
    "with torch.no_grad(): nf = encoder(torch.randn(2,3,size,size)).size(-1)\n",
    "classifier = create_cls_module(nf, dls.c)\n",
    "model = nn.Sequential(encoder, classifier)\n",
    "cbs = [SaveModelCallback(every_epoch=True, fname=f\"{arch}_size{size}_lr{lr}_epochs{epochs}\")]\n",
    "if WANDB: cbs += [WandbCallback(log_preds=False,log_model=False)]\n",
    "learn = Learner(dls, model, opt_func=ranger, cbs=cbs\n",
    "                metrics=[f1macro, f1micro], loss_func=BCEWithLogitsLossFlat())\n",
    "learn.to_fp16();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_flat_cos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.03630780577659607, lr_steep=0.3019951581954956)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WANDB: wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
