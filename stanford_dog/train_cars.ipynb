{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from self_supervised.layers import *\n",
    "import sklearn\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_vit import *\n",
    "from utils.attention import *\n",
    "from utils.object_crops import *\n",
    "from utils.part_crops import *\n",
    "from utils.multi_crop_model import *\n",
    "\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path(\"../data/stanford-cars-dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#7) [Path('../data/stanford-cars-dataset/cars_annos.mat'),Path('../data/stanford-cars-dataset/cars_meta.mat'),Path('../data/stanford-cars-dataset/cars_test_annos.mat'),Path('../data/stanford-cars-dataset/cars_test_annos_withlabels.mat'),Path('../data/stanford-cars-dataset/cars_train'),Path('../data/stanford-cars-dataset/cars_train_annos.mat'),Path('../data/stanford-cars-dataset/cars_test')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = get_image_files(datapath/'cars_train')\n",
    "test_filenames = get_image_files(datapath/'cars_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8144, 8041)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_filenames), len(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annos = loadmat(datapath/'cars_train_annos.mat')['annotations'][0]\n",
    "test_annos = loadmat(datapath/'cars_test_annos.mat')['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8144, 8041)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_annos), len(test_annos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn2label = {}\n",
    "for anno in train_annos:\n",
    "    class_id = int(anno[-2].item())\n",
    "    fname = anno[-1].item()\n",
    "    train_fn2label['cars_train/'+fname] = class_id\n",
    "    \n",
    "test_fn2label = {}\n",
    "for anno in test_annos:\n",
    "    class_id = int(anno[-2].item())\n",
    "    fname = anno[-1].item()\n",
    "    test_fn2label['cars_test/'+fname] = class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8144, 8041)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_fn2label), len(test_fn2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = train_filenames+test_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn2label = {**train_fn2label, **test_fn2label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(filename):      return PILImage.create(filename)\n",
    "def read_image_size(filename): return PILImage.create(filename).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16185, 16185)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames), len(fn2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cars_test/00429.jpg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_filenames[0].parent.name+'/'+test_filenames[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label(filename): return fn2label[filename.parent.name+'/'+filename.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = np.random.choice(filenames,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "size,bs = 512,16\n",
    "\n",
    "tfms = [[read_image, ToTensor, RandomResizedCrop(size, min_scale=.7)], \n",
    "        [read_label, Categorize()]]\n",
    "\n",
    "valid_splitter = lambda o: True if o in test_filenames else False \n",
    "dsets = Datasets(filenames, tfms=tfms, splits=FuncSplitter(valid_splitter)(filenames))\n",
    "\n",
    "batch_augs = aug_transforms()\n",
    "\n",
    "stats = imagenet_stats\n",
    "\n",
    "batch_tfms = [IntToFloatTensor] + batch_augs + [Normalize.from_stats(*stats)]\n",
    "dls = dsets.dataloaders(bs=bs, after_batch=batch_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_bil(x,sz): return F.interpolate(x,mode='bilinear',align_corners=True, size=(sz,sz))\n",
    "\n",
    "def apply_attn_erasing(x, attn_maps, thresh, p=0.5): \n",
    "    \"x: bs x c x h x w, attn_maps: bs x h x w\"\n",
    "    erasing_mask = (attn_maps>thresh).unsqueeze(1)\n",
    "    ps = torch.zeros(erasing_mask.size(0)).float().bernoulli(p).to(erasing_mask.device)\n",
    "    rand_erasing_mask = 1-erasing_mask*ps[...,None,None,None]\n",
    "    return rand_erasing_mask*x\n",
    "\n",
    "class ViTEncoder(Module):\n",
    "    \"Timm ViT encoder which return encoder outputs and optionally returns attention weights with gradient checkpointing\"\n",
    "    def __init__(self, vit, nblocks=12, checkpoint_nchunks=2, return_attn_wgts=True):\n",
    "                \n",
    "        # initialize params\n",
    "        self.patch_embed = vit.patch_embed\n",
    "        self.cls_token = vit.cls_token\n",
    "        self.pos_embed = vit.pos_embed\n",
    "        self.pos_drop = vit.pos_drop\n",
    "        \n",
    "        # until any desired layers\n",
    "        self.blocks = vit.blocks[:nblocks]        \n",
    "        \n",
    "        # gradient checkpointing\n",
    "        self.checkpoint_nchunks = checkpoint_nchunks\n",
    "        \n",
    "        # return attention weights from L layers\n",
    "        self.return_attn_wgts = return_attn_wgts\n",
    "         \n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "\n",
    "        # collect attn_wgts from all layers\n",
    "        if self.return_attn_wgts:\n",
    "            attn_wgts = []\n",
    "            for i,blk in enumerate(self.blocks):\n",
    "                if i<self.checkpoint_nchunks: x,attn_wgt = checkpoint(blk, x)\n",
    "                else:                         x,attn_wgt = blk(x)\n",
    "                attn_wgts.append(attn_wgt)\n",
    "            return x,attn_wgts\n",
    "        \n",
    "        else:\n",
    "            for i,blk in enumerate(self.blocks):\n",
    "                if i<self.checkpoint_nchunks: x,_ = checkpoint(blk, x)\n",
    "                else:                         x,_ = blk(x)\n",
    "            return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.forward_features(x)\n",
    "    \n",
    "    \n",
    "class MultiCropViT(Module):\n",
    "    \"Multi Scale Multi Crop ViT Model\"\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 num_classes,\n",
    "                 input_res=384, high_res=786, min_obj_area=112*112, crop_sz=224,\n",
    "                 crop_object=True, crop_object_parts=True,\n",
    "                 do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7,\n",
    "                 encoder_nblocks=12, checkpoint_nchunks=12):\n",
    "        \n",
    "        store_attr()\n",
    "\n",
    "        self.image_encoder = ViTEncoder(encoder, nblocks=encoder_nblocks, checkpoint_nchunks=checkpoint_nchunks)\n",
    "        self.norm = partial(nn.LayerNorm, eps=1e-6)(768)        \n",
    "        self.classifier = create_cls_module(768, num_classes, lin_ftrs=[768], use_bn=False, first_bn=False, ps=0.)\n",
    "    \n",
    "        \n",
    "    def forward(self, xb_high_res):\n",
    "\n",
    "        # get full image attention weigths / feature\n",
    "        self.image_encoder.return_attn_wgts = True\n",
    "        xb_input_res = F.interpolate(xb_high_res, size=(self.input_res,self.input_res))\n",
    "        _, attn_wgts = self.image_encoder(xb_input_res)\n",
    "        self.image_encoder.return_attn_wgts = False\n",
    "        \n",
    "        # get attention maps\n",
    "        attn_maps = generate_batch_attention_maps(attn_wgts, None, mode=None).detach()\n",
    "        attn_maps_high_res = interpolate_bil(attn_maps[None,...],self.high_res)[0]\n",
    "        attn_maps_input_res = interpolate_bil(attn_maps[None,...],self.input_res)[0]\n",
    "        \n",
    "\n",
    "        \n",
    "        #### ORIGINAL IMAGE ####\n",
    "        # original image attention erasing and features\n",
    "        if (self.training and self.do_attn_erasing):\n",
    "            xb_input_res = apply_attn_erasing(xb_input_res, attn_maps_input_res, self.attn_erasing_thresh, self.p_attn_erasing)\n",
    "        x_full = self.image_encoder(xb_input_res)\n",
    "\n",
    "        \n",
    "        \n",
    "        #### OBJECT CROP ####        \n",
    "        if self.crop_object:\n",
    "            # get object bboxes\n",
    "            batch_object_bboxes = np.vstack([generate_attention_coordinates(attn_map, \n",
    "                                                                            num_bboxes=1,\n",
    "                                                                            min_area=self.min_obj_area,\n",
    "                                                                            random_crop_sz=self.input_res)\n",
    "                                                    for attn_map in to_np(attn_maps_high_res)])\n",
    "            # crop objects\n",
    "            xb_objects, attn_maps_objects = [], []\n",
    "            for i, obj_bbox in enumerate(batch_object_bboxes):\n",
    "                minr, minc, maxr, maxc = obj_bbox\n",
    "                xb_objects        += [interpolate_bil(xb_high_res[i][:,minr:maxr,minc:maxc][None,...],self.input_res)[0]]\n",
    "                attn_maps_objects += [interpolate_bil(attn_maps_high_res[i][minr:maxr,minc:maxc][None,None,...],self.input_res)[0][0]]\n",
    "            xb_objects,attn_maps_objects = torch.stack(xb_objects),torch.stack(attn_maps_objects)\n",
    "\n",
    "            # object image attention erasing and features\n",
    "            if (self.training and self.do_attn_erasing):\n",
    "                xb_objects = apply_attn_erasing(xb_objects, attn_maps_objects, self.attn_erasing_thresh, self.p_attn_erasing)\n",
    "            x_object = self.image_encoder(xb_objects)\n",
    "                    \n",
    "        \n",
    "\n",
    "        #### OBJECT CROP PARTS ####\n",
    "        if self.crop_object_parts:\n",
    "            #get object crop bboxes\n",
    "            small_attn_maps_objects = interpolate_bil(attn_maps_objects[None,],self.input_res//3)[0] # to speed up calculation\n",
    "            batch_crop_bboxes = generate_batch_crops(small_attn_maps_objects.cpu(),\n",
    "                                                     source_sz=self.input_res//3, \n",
    "                                                     targ_sz=self.input_res, \n",
    "                                                     targ_bbox_sz=self.crop_sz,\n",
    "                                                     num_bboxes=2,\n",
    "                                                     nms_thresh=0.1)\n",
    "\n",
    "            # crop object parts\n",
    "            xb_crops1,xb_crops2 = [],[]\n",
    "            for i, crop_bboxes in enumerate(batch_crop_bboxes):\n",
    "                minr, minc, maxr, maxc = crop_bboxes[0]\n",
    "                xb_crops1 += [interpolate_bil(xb_objects[i][:,minr:maxr,minc:maxc][None,...],self.input_res)[0]]\n",
    "                minr, minc, maxr, maxc = crop_bboxes[1]\n",
    "                xb_crops2 += [interpolate_bil(xb_objects[i][:,minr:maxr,minc:maxc][None,...],self.input_res)[0]]\n",
    "            xb_crops1,xb_crops2 = torch.stack(xb_crops1),torch.stack(xb_crops2)\n",
    "\n",
    "            # crop features\n",
    "            x_crops1 = self.image_encoder(xb_crops1)\n",
    "            x_crops2 = self.image_encoder(xb_crops2)\n",
    "        \n",
    "        \n",
    "        # predict\n",
    "        x_full = self.norm(x_full)[:,0]\n",
    "        if self.crop_object:\n",
    "            x_object = self.norm(x_object)[:,0]\n",
    "            if self.crop_object_parts:\n",
    "                x_crops1 = self.norm(x_crops1)[:,0]\n",
    "                x_crops2 = self.norm(x_crops2)[:,0]\n",
    "                return self.classifier(x_full), self.classifier(x_object), self.classifier(x_crops1), self.classifier(x_crops2)\n",
    "            return self.classifier(x_full), self.classifier(x_object)\n",
    "        return  self.classifier(x_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_splitter(m): return L(m.image_encoder, m.norm, m.classifier).map(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFuncA(Module): # only object\n",
    "    def __init__(self):               self.lf = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "    def forward(self, preds, targs):  return self.lf(preds[1],targs)\n",
    "    \n",
    "class LossFuncB(Module): # full + object\n",
    "    def __init__(self):               self.lf = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "    def forward(self, preds, targs):  return self.lf(preds[0],targs) + self.lf(preds[1],targs)\n",
    "    \n",
    "class LossFuncC(Module): # full + object + crops\n",
    "    def __init__(self):               self.lf = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "    def forward(self, preds, targs):  return self.lf(preds[0],targs) + self.lf(preds[1],targs) + (self.lf(preds[2],targs)+self.lf(preds[3],targs))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyA(preds, targs): return accuracy(preds[0], targs) # full\n",
    "def accuracyB(preds, targs): return accuracy(preds[1], targs) # full, object\n",
    "def accuracyC(preds, targs): return accuracy((preds[2]+preds[3])/2, targs) # full, object, crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkeremturgutlu\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.25 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">pious-universe-55</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/keremturgutlu/fgvc-2021\" target=\"_blank\">https://wandb.ai/keremturgutlu/fgvc-2021</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/keremturgutlu/fgvc-2021/runs/39vm6fj7\" target=\"_blank\">https://wandb.ai/keremturgutlu/fgvc-2021/runs/39vm6fj7</a><br/>\n",
       "                Run data is saved locally in <code>/home/code-base/fgvc/stanford_dog/wandb/run-20210407_062313-39vm6fj7</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [0,4,6]:\n",
    "\n",
    "    if i == 0:\n",
    "        # exp 1 - full image\n",
    "        model_config = dict(crop_object=False, crop_object_parts=False, do_attn_erasing=False, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "        metrics =[accuracy] \n",
    "\n",
    "    if i == 1:\n",
    "        # exp 2 - full image\n",
    "        model_config = dict(crop_object=False, crop_object_parts=False, do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "        metrics =[accuracy] \n",
    "\n",
    "    if i == 2:\n",
    "        # exp 3 - object\n",
    "        model_config = dict(crop_object=True, crop_object_parts=False, do_attn_erasing=False, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncA()\n",
    "        metrics =[accuracyB] \n",
    "\n",
    "    if i == 3:\n",
    "        # exp 4 - object\n",
    "        model_config = dict(crop_object=True, crop_object_parts=False, do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncA()\n",
    "        metrics =[accuracyB] \n",
    "\n",
    "    if i == 4:\n",
    "        # exp 5 - full image + object\n",
    "        model_config = dict(crop_object=True, crop_object_parts=False, do_attn_erasing=False, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncB()\n",
    "        metrics =[accuracyA, accuracyB] \n",
    "\n",
    "    if i == 5:\n",
    "        # exp 6 - full image + object\n",
    "        model_config = dict(crop_object=True, crop_object_parts=False, do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncB()\n",
    "        metrics =[accuracyA, accuracyB]\n",
    "\n",
    "    if i == 6:\n",
    "        # exp 7 - full image + object + crops\n",
    "        model_config = dict(crop_object=True, crop_object_parts=True, do_attn_erasing=False, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncC()\n",
    "        metrics =[accuracyA, accuracyB, accuracyC]\n",
    "\n",
    "    if i == 7:\n",
    "        # exp 8 - full image + object + crops\n",
    "        model_config = dict(crop_object=True, crop_object_parts=True, do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncC()\n",
    "        metrics =[accuracyA, accuracyB, accuracyC]\n",
    "\n",
    "    # modified timm vit encoder\n",
    "    arch = \"vit_base_patch16_384\"\n",
    "    _encoder = create_encoder(arch, pretrained=True, n_in=3)\n",
    "    encoder = VisionTransformer(img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12)\n",
    "    encoder.head = Identity()\n",
    "    encoder.load_state_dict(_encoder.state_dict());\n",
    "    \n",
    "    \n",
    "    high_res=size\n",
    "    min_obj_area=(high_res//7)*(high_res//7)\n",
    "    crop_sz=high_res//3\n",
    "    \n",
    "\n",
    "    mcvit_model = MultiCropViT(encoder, num_classes=dls.c, input_res=384, high_res=high_res, min_obj_area=min_obj_area, crop_sz=crop_sz,\n",
    "                                 encoder_nblocks=12, checkpoint_nchunks=12, **model_config)\n",
    "\n",
    "    WANDB = True\n",
    "    if WANDB:\n",
    "        xtra_config = model_config\n",
    "        xtra_config.update({\"Dataset\":\"FGVC-Cars\"})\n",
    "        wandb.init(project=\"fgvc-2021\", config=xtra_config);\n",
    "\n",
    "    cbs = []\n",
    "    if WANDB: cbs += [WandbCallback(log_preds=False,log_model=False)]\n",
    "    learn = Learner(dls, mcvit_model, opt_func=ranger, cbs=cbs, metrics=metrics, loss_func=loss_func, splitter=model_splitter)\n",
    "    learn.to_fp16();\n",
    "    \n",
    "    epochs = 4\n",
    "    lr = 3e-3\n",
    "    \n",
    "    learn.freeze_to(1)\n",
    "    learn.fit_one_cycle(epochs, lr_max=(lr), pct_start=0.5)\n",
    "\n",
    "    lr /= 3\n",
    "    learn.unfreeze()\n",
    "    learn.fit_one_cycle(int(epochs**2), lr_max=[lr/25,lr,lr], pct_start=0.5)\n",
    "    \n",
    "    del learn, encoder, mcvit_model\n",
    "    gc.collect()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = learn.model.cuda()(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 120])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
