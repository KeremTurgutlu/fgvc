{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from self_supervised.layers import *\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_vit import *\n",
    "from utils.attention import *\n",
    "from utils.object_crops import *\n",
    "from utils.part_crops import *\n",
    "from utils.multi_crop_model import *\n",
    "\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path(\"../data/fgvc-aircraft/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(datapath/'train.csv')\n",
    "valid_df = pd.read_csv(datapath/'val.csv')\n",
    "test_df = pd.read_csv(datapath/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3334, 3), (3333, 3), (3333, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>Classes</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1025794.jpg</td>\n",
       "      <td>707-320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1340192.jpg</td>\n",
       "      <td>707-320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0056978.jpg</td>\n",
       "      <td>707-320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0698580.jpg</td>\n",
       "      <td>707-320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0450014.jpg</td>\n",
       "      <td>707-320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename  Classes  Labels\n",
       "0  1025794.jpg  707-320       0\n",
       "1  1340192.jpg  707-320       0\n",
       "2  0056978.jpg  707-320       0\n",
       "3  0698580.jpg  707-320       0\n",
       "4  0450014.jpg  707-320       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10000) ['0847415.jpg','1053510.jpg','0174925.jpg','1843606.jpg','1446516.jpg','0690072.jpg','1231579.jpg','1164220.jpg','2217846.jpg','0323097.jpg'...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(datapath/'fgvc-aircraft-2013b/fgvc-aircraft-2013b/data/images').ls().map(lambda o: o.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(filename): return PILImage.create(datapath/'fgvc-aircraft-2013b/fgvc-aircraft-2013b/data/images'/filename)\n",
    "def read_image_size(filename): return PILImage.create(datapath/'fgvc-aircraft-2013b/fgvc-aircraft-2013b/data/images'/filename).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fns, valid_fns, test_fns = train_df['filename'].values,valid_df['filename'].values,test_df['filename'].values\n",
    "filenames = np.concatenate([train_fns, valid_fns, test_fns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn2label = {**dict(zip(train_df['filename'],train_df['Classes'])),\n",
    "            **dict(zip(valid_df['filename'],valid_df['Classes'])),\n",
    "            **dict(zip(test_df['filename'],test_df['Classes']))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames), len(fn2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label(filename): return fn2label[filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_sizes = parallel(read_image_size, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter(img_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_filenames = sample_df.query(\"split == 'valid'\")['filename'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames = np.random.choice(filenames, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "size,bs = 786,16\n",
    "\n",
    "tfms = [[read_image, ToTensor, RandomResizedCrop(size, min_scale=.75)], \n",
    "        [read_label, Categorize()]]\n",
    "\n",
    "valid_splitter = lambda o: True if o in test_fns else False \n",
    "dsets = Datasets(filenames, tfms=tfms, splits=FuncSplitter(valid_splitter)(filenames))\n",
    "\n",
    "batch_augs = aug_transforms()\n",
    "\n",
    "stats = imagenet_stats\n",
    "\n",
    "batch_tfms = [IntToFloatTensor] + batch_augs + [Normalize.from_stats(*stats)]\n",
    "dls = dsets.dataloaders(bs=bs, after_batch=batch_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6667, 3333)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dls.train_ds), len(dls.valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_bil(x,sz): return F.interpolate(x,mode='bilinear',align_corners=True, size=(sz,sz))\n",
    "\n",
    "def apply_attn_erasing(x, attn_maps, thresh, p=0.5): \n",
    "    \"x: bs x c x h x w, attn_maps: bs x h x w\"\n",
    "    erasing_mask = (attn_maps>thresh).unsqueeze(1)\n",
    "    ps = torch.zeros(erasing_mask.size(0)).float().bernoulli(p).to(erasing_mask.device)\n",
    "    rand_erasing_mask = 1-erasing_mask*ps[...,None,None,None]\n",
    "    return rand_erasing_mask*x\n",
    "\n",
    "class ViTEncoder(Module):\n",
    "    \"Timm ViT encoder which return encoder outputs and optionally returns attention weights with gradient checkpointing\"\n",
    "    def __init__(self, vit, nblocks=12, checkpoint_nchunks=2, return_attn_wgts=True):\n",
    "                \n",
    "        # initialize params\n",
    "        self.patch_embed = vit.patch_embed\n",
    "        self.cls_token = vit.cls_token\n",
    "        self.pos_embed = vit.pos_embed\n",
    "        self.pos_drop = vit.pos_drop\n",
    "        \n",
    "        # until any desired layers\n",
    "        self.blocks = vit.blocks[:nblocks]        \n",
    "        \n",
    "        # gradient checkpointing\n",
    "        self.checkpoint_nchunks = checkpoint_nchunks\n",
    "        \n",
    "        # return attention weights from L layers\n",
    "        self.return_attn_wgts = return_attn_wgts\n",
    "         \n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "\n",
    "        # collect attn_wgts from all layers\n",
    "        if self.return_attn_wgts:\n",
    "            attn_wgts = []\n",
    "            for i,blk in enumerate(self.blocks):\n",
    "                if i<self.checkpoint_nchunks: x,attn_wgt = checkpoint(blk, x)\n",
    "                else:                         x,attn_wgt = blk(x)\n",
    "                attn_wgts.append(attn_wgt)\n",
    "            return x,attn_wgts\n",
    "        \n",
    "        else:\n",
    "            for i,blk in enumerate(self.blocks):\n",
    "                if i<self.checkpoint_nchunks: x,_ = checkpoint(blk, x)\n",
    "                else:                         x,_ = blk(x)\n",
    "            return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.forward_features(x)\n",
    "    \n",
    "    \n",
    "class MultiCropViT(Module):\n",
    "    \"Multi Scale Multi Crop ViT Model\"\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 num_classes,\n",
    "                 input_res=384, high_res=786, min_obj_area=112*112, crop_sz=224,\n",
    "                 crop_object=True, crop_object_parts=True,\n",
    "                 do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7,\n",
    "                 encoder_nblocks=12, checkpoint_nchunks=12):\n",
    "        \n",
    "        store_attr()\n",
    "\n",
    "        self.image_encoder = ViTEncoder(encoder, nblocks=encoder_nblocks, checkpoint_nchunks=checkpoint_nchunks)\n",
    "        self.norm = partial(nn.LayerNorm, eps=1e-6)(768)        \n",
    "        self.classifier = create_cls_module(768, num_classes, lin_ftrs=[768], use_bn=False, first_bn=False, ps=0.)\n",
    "    \n",
    "        \n",
    "    def forward(self, xb_high_res):\n",
    "\n",
    "        # get full image attention weigths / feature\n",
    "        self.image_encoder.return_attn_wgts = True\n",
    "        xb_input_res = F.interpolate(xb_high_res, size=(self.input_res,self.input_res))\n",
    "        _, attn_wgts = self.image_encoder(xb_input_res)\n",
    "        self.image_encoder.return_attn_wgts = False\n",
    "        \n",
    "        # get attention maps\n",
    "        attn_maps = generate_batch_attention_maps(attn_wgts, None, mode=None).detach()\n",
    "        attn_maps_high_res = interpolate_bil(attn_maps[None,...],self.high_res)[0]\n",
    "        attn_maps_input_res = interpolate_bil(attn_maps[None,...],self.input_res)[0]\n",
    "        \n",
    "\n",
    "        \n",
    "        #### ORIGINAL IMAGE ####\n",
    "        # original image attention erasing and features\n",
    "        if (self.training and self.do_attn_erasing):\n",
    "            xb_input_res = apply_attn_erasing(xb_input_res, attn_maps_input_res, self.attn_erasing_thresh, self.p_attn_erasing)\n",
    "        x_full = self.image_encoder(xb_input_res)\n",
    "\n",
    "        \n",
    "        \n",
    "        #### OBJECT CROP ####        \n",
    "        if self.crop_object:\n",
    "            # get object bboxes\n",
    "            batch_object_bboxes = np.vstack([generate_attention_coordinates(attn_map, \n",
    "                                                                            num_bboxes=1,\n",
    "                                                                            min_area=self.min_obj_area,\n",
    "                                                                            random_crop_sz=self.input_res)\n",
    "                                                    for attn_map in to_np(attn_maps_high_res)])\n",
    "            # crop objects\n",
    "            xb_objects, attn_maps_objects = [], []\n",
    "            for i, obj_bbox in enumerate(batch_object_bboxes):\n",
    "                minr, minc, maxr, maxc = obj_bbox\n",
    "                xb_objects        += [interpolate_bil(xb_high_res[i][:,minr:maxr,minc:maxc][None,...],self.input_res)[0]]\n",
    "                attn_maps_objects += [interpolate_bil(attn_maps_high_res[i][minr:maxr,minc:maxc][None,None,...],self.input_res)[0][0]]\n",
    "            xb_objects,attn_maps_objects = torch.stack(xb_objects),torch.stack(attn_maps_objects)\n",
    "\n",
    "            # object image attention erasing and features\n",
    "            if (self.training and self.do_attn_erasing):\n",
    "                xb_objects = apply_attn_erasing(xb_objects, attn_maps_objects, self.attn_erasing_thresh, self.p_attn_erasing)\n",
    "            x_object = self.image_encoder(xb_objects)\n",
    "                    \n",
    "        \n",
    "\n",
    "        #### OBJECT CROP PARTS ####\n",
    "        if self.crop_object_parts:\n",
    "            #get object crop bboxes\n",
    "            small_attn_maps_objects = interpolate_bil(attn_maps_objects[None,],self.input_res//3)[0] # to speed up calculation\n",
    "            batch_crop_bboxes = generate_batch_crops(small_attn_maps_objects.cpu(),\n",
    "                                                     source_sz=self.input_res//3, \n",
    "                                                     targ_sz=self.input_res, \n",
    "                                                     targ_bbox_sz=self.crop_sz,\n",
    "                                                     num_bboxes=2,\n",
    "                                                     nms_thresh=0.1)\n",
    "\n",
    "            # crop object parts\n",
    "            xb_crops1,xb_crops2 = [],[]\n",
    "            for i, crop_bboxes in enumerate(batch_crop_bboxes):\n",
    "                minr, minc, maxr, maxc = crop_bboxes[0]\n",
    "                xb_crops1 += [interpolate_bil(xb_objects[i][:,minr:maxr,minc:maxc][None,...],self.input_res)[0]]\n",
    "                minr, minc, maxr, maxc = crop_bboxes[1]\n",
    "                xb_crops2 += [interpolate_bil(xb_objects[i][:,minr:maxr,minc:maxc][None,...],self.input_res)[0]]\n",
    "            xb_crops1,xb_crops2 = torch.stack(xb_crops1),torch.stack(xb_crops2)\n",
    "\n",
    "            # crop features\n",
    "            x_crops1 = self.image_encoder(xb_crops1)\n",
    "            x_crops2 = self.image_encoder(xb_crops2)\n",
    "        \n",
    "        \n",
    "        # predict\n",
    "        x_full = self.norm(x_full)[:,0]\n",
    "        if self.crop_object:\n",
    "            x_object = self.norm(x_object)[:,0]\n",
    "            if self.crop_object_parts:\n",
    "                x_crops1 = self.norm(x_crops1)[:,0]\n",
    "                x_crops2 = self.norm(x_crops2)[:,0]\n",
    "                return self.classifier(x_full), self.classifier(x_object), self.classifier(x_crops1), self.classifier(x_crops2)\n",
    "            return self.classifier(x_full), self.classifier(x_object)\n",
    "        return  self.classifier(x_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_splitter(m): return L(m.image_encoder, m.norm, m.classifier).map(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFuncA(Module): # only object\n",
    "    def __init__(self):               self.lf = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "    def forward(self, preds, targs):  return self.lf(preds[1],targs)\n",
    "    \n",
    "class LossFuncB(Module): # full + object\n",
    "    def __init__(self):               self.lf = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "    def forward(self, preds, targs):  return self.lf(preds[0],targs) + self.lf(preds[1],targs)\n",
    "    \n",
    "class LossFuncC(Module): # full + object + crops\n",
    "    def __init__(self):               self.lf = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "    def forward(self, preds, targs):  return self.lf(preds[0],targs) + self.lf(preds[1],targs) + (self.lf(preds[2],targs)+self.lf(preds[3],targs))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyA(preds, targs): return accuracy(preds[0], targs) # full\n",
    "def accuracyB(preds, targs): return accuracy(preds[1], targs) # full, object\n",
    "def accuracyC(preds, targs): return accuracy((preds[2]+preds[3])/2, targs) # full, object, crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkeremturgutlu\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.25 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">playful-pine-57</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/keremturgutlu/fgvc-2021\" target=\"_blank\">https://wandb.ai/keremturgutlu/fgvc-2021</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/keremturgutlu/fgvc-2021/runs/3mb87h7u\" target=\"_blank\">https://wandb.ai/keremturgutlu/fgvc-2021/runs/3mb87h7u</a><br/>\n",
       "                Run data is saved locally in <code>/home/code-base/fgvc/stanford_dog/wandb/run-20210407_064521-3mb87h7u</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.637590</td>\n",
       "      <td>3.557274</td>\n",
       "      <td>0.169517</td>\n",
       "      <td>02:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.179717</td>\n",
       "      <td>3.146391</td>\n",
       "      <td>0.230723</td>\n",
       "      <td>02:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.869263</td>\n",
       "      <td>2.788370</td>\n",
       "      <td>0.348335</td>\n",
       "      <td>02:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.443424</td>\n",
       "      <td>2.555439</td>\n",
       "      <td>0.431443</td>\n",
       "      <td>02:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.039598</td>\n",
       "      <td>2.369647</td>\n",
       "      <td>0.504350</td>\n",
       "      <td>02:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.795099</td>\n",
       "      <td>2.301403</td>\n",
       "      <td>0.517552</td>\n",
       "      <td>02:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='16' class='' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      44.44% [16/36 1:14:52<1:33:35]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.676557</td>\n",
       "      <td>2.263818</td>\n",
       "      <td>0.531953</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.665952</td>\n",
       "      <td>2.206906</td>\n",
       "      <td>0.549355</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.645507</td>\n",
       "      <td>2.155257</td>\n",
       "      <td>0.570657</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.611038</td>\n",
       "      <td>2.129806</td>\n",
       "      <td>0.575158</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.532869</td>\n",
       "      <td>2.091775</td>\n",
       "      <td>0.591659</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.446730</td>\n",
       "      <td>2.031212</td>\n",
       "      <td>0.614761</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.389376</td>\n",
       "      <td>1.977178</td>\n",
       "      <td>0.636064</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.316698</td>\n",
       "      <td>1.967219</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>04:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.243147</td>\n",
       "      <td>1.966739</td>\n",
       "      <td>0.638464</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.202027</td>\n",
       "      <td>1.947852</td>\n",
       "      <td>0.648965</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.142180</td>\n",
       "      <td>1.861410</td>\n",
       "      <td>0.681968</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.099198</td>\n",
       "      <td>1.913655</td>\n",
       "      <td>0.665166</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.059157</td>\n",
       "      <td>1.913051</td>\n",
       "      <td>0.669967</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.030893</td>\n",
       "      <td>1.881339</td>\n",
       "      <td>0.669667</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.030746</td>\n",
       "      <td>1.843359</td>\n",
       "      <td>0.692169</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.996640</td>\n",
       "      <td>1.994713</td>\n",
       "      <td>0.649565</td>\n",
       "      <td>04:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='346' class='' max='416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      83.17% [346/416 03:11<00:38 0.9599]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [0,4,6]:\n",
    "\n",
    "    if i == 0:\n",
    "        # exp 1 - full image\n",
    "        model_config = dict(crop_object=False, crop_object_parts=False, do_attn_erasing=False, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "        metrics =[accuracy] \n",
    "\n",
    "    if i == 1:\n",
    "        # exp 2 - full image\n",
    "        model_config = dict(crop_object=False, crop_object_parts=False, do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "        metrics =[accuracy] \n",
    "\n",
    "    if i == 2:\n",
    "        # exp 3 - object\n",
    "        model_config = dict(crop_object=True, crop_object_parts=False, do_attn_erasing=False, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncA()\n",
    "        metrics =[accuracyB] \n",
    "\n",
    "    if i == 3:\n",
    "        # exp 4 - object\n",
    "        model_config = dict(crop_object=True, crop_object_parts=False, do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncA()\n",
    "        metrics =[accuracyB] \n",
    "\n",
    "    if i == 4:\n",
    "        # exp 5 - full image + object\n",
    "        model_config = dict(crop_object=True, crop_object_parts=False, do_attn_erasing=False, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncB()\n",
    "        metrics =[accuracyA, accuracyB] \n",
    "\n",
    "    if i == 5:\n",
    "        # exp 6 - full image + object\n",
    "        model_config = dict(crop_object=True, crop_object_parts=False, do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncB()\n",
    "        metrics =[accuracyA, accuracyB]\n",
    "\n",
    "    if i == 6:\n",
    "        # exp 7 - full image + object + crops\n",
    "        model_config = dict(crop_object=True, crop_object_parts=True, do_attn_erasing=False, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncC()\n",
    "        metrics =[accuracyA, accuracyB, accuracyC]\n",
    "\n",
    "    if i == 7:\n",
    "        # exp 8 - full image + object + crops\n",
    "        model_config = dict(crop_object=True, crop_object_parts=True, do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncC()\n",
    "        metrics =[accuracyA, accuracyB, accuracyC]\n",
    "\n",
    "    # modified timm vit encoder\n",
    "    arch = \"vit_base_patch16_384\"\n",
    "    _encoder = create_encoder(arch, pretrained=True, n_in=3)\n",
    "    encoder = VisionTransformer(img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12)\n",
    "    encoder.head = Identity()\n",
    "    encoder.load_state_dict(_encoder.state_dict());\n",
    "\n",
    "    mcvit_model = MultiCropViT(encoder, num_classes=dls.c, input_res=384, high_res=786, min_obj_area=128*128, crop_sz=224,\n",
    "                                 encoder_nblocks=12, checkpoint_nchunks=12, **model_config)\n",
    "\n",
    "    WANDB = True\n",
    "    if WANDB:\n",
    "        xtra_config = model_config\n",
    "        xtra_config.update({\"Dataset\":\"FGVC-Aircraft\"})\n",
    "        wandb.init(project=\"fgvc-2021\", config=xtra_config);\n",
    "\n",
    "    cbs = []\n",
    "    if WANDB: cbs += [WandbCallback(log_preds=False,log_model=False)]\n",
    "    learn = Learner(dls, mcvit_model, opt_func=ranger, cbs=cbs, metrics=metrics, loss_func=loss_func, splitter=model_splitter)\n",
    "    learn.to_fp16();\n",
    "    \n",
    "    epochs = 6\n",
    "    lr = 3e-3\n",
    "    \n",
    "    learn.freeze_to(1)\n",
    "    learn.fit_one_cycle(epochs, lr_max=(lr), pct_start=0.5)\n",
    "\n",
    "    lr /= 3\n",
    "    learn.unfreeze()\n",
    "    learn.fit_one_cycle(int(epochs**2), lr_max=[lr/25,lr,lr], pct_start=0.5)\n",
    "    \n",
    "    del learn, encoder, mcvit_model\n",
    "    gc.collect()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
