{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from self_supervised.layers import *\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_vit import *\n",
    "from utils.attention import *\n",
    "from utils.object_crops import *\n",
    "from utils.part_crops import *\n",
    "from utils.multi_crop_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callback.wandb import WandbCallback\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path(\"../data/stanford-dogs-dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(datapath/'train.csv')\n",
    "test_df = pd.read_csv(datapath/'test.csv')\n",
    "sample_df = pd.read_csv(datapath/'sample_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12000, 2), (8580, 2), (6000, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape, sample_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(filename): return PILImage.create(datapath/'images/Images'/filename)\n",
    "def read_image_size(filename): return PILImage.create(datapath/'images/Images'/filename).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FAST:\n",
    "    filenames = sample_df['filename'].values\n",
    "    labels = sample_df['label'].values\n",
    "    fn2label = dict(zip(filenames, labels))\n",
    "else:\n",
    "    filenames = train_df['filenames'].values\n",
    "    labels = train_df['labels'].values\n",
    "    fn2label = dict(zip(filenames, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label(filename): return fn2label[filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_filenames = sample_df.query(\"split == 'valid'\")['filename'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "size,bs = 448,16\n",
    "\n",
    "tfms = [[read_image, ToTensor, RandomResizedCrop(size, min_scale=.75)], \n",
    "        [read_label, Categorize()]]\n",
    "\n",
    "valid_splitter = lambda o: True if o in valid_filenames else False \n",
    "dsets = Datasets(filenames, tfms=tfms, splits=FuncSplitter(valid_splitter)(filenames))\n",
    "\n",
    "batch_augs = aug_transforms()\n",
    "\n",
    "stats = imagenet_stats\n",
    "\n",
    "batch_tfms = [IntToFloatTensor] + batch_augs + [Normalize.from_stats(*stats)]\n",
    "dls = dsets.dataloaders(bs=bs, after_batch=batch_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_bil(x,sz): return F.interpolate(x,mode='bilinear',align_corners=True, size=(sz,sz))\n",
    "\n",
    "def apply_attn_erasing(x, attn_maps, thresh, p=0.5): \n",
    "    \"x: bs x c x h x w, attn_maps: bs x h x w\"\n",
    "    erasing_mask = (attn_maps>thresh).unsqueeze(1)\n",
    "    ps = torch.zeros(erasing_mask.size(0)).float().bernoulli(p).to(erasing_mask.device)\n",
    "    rand_erasing_mask = 1-erasing_mask*ps[...,None,None,None]\n",
    "    return rand_erasing_mask*x\n",
    "\n",
    "class ViTEncoder(Module):\n",
    "    \"Timm ViT encoder which return encoder outputs and optionally returns attention weights with gradient checkpointing\"\n",
    "    def __init__(self, vit, nblocks=12, checkpoint_nchunks=2, return_attn_wgts=True):\n",
    "                \n",
    "        # initialize params\n",
    "        self.patch_embed = vit.patch_embed\n",
    "        self.cls_token = vit.cls_token\n",
    "        self.pos_embed = vit.pos_embed\n",
    "        self.pos_drop = vit.pos_drop\n",
    "        \n",
    "        # until any desired layers\n",
    "        self.blocks = vit.blocks[:nblocks]        \n",
    "        \n",
    "        # gradient checkpointing\n",
    "        self.checkpoint_nchunks = checkpoint_nchunks\n",
    "        \n",
    "        # return attention weights from L layers\n",
    "        self.return_attn_wgts = return_attn_wgts\n",
    "         \n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "\n",
    "        # collect attn_wgts from all layers\n",
    "        if self.return_attn_wgts:\n",
    "            attn_wgts = []\n",
    "            for i,blk in enumerate(self.blocks):\n",
    "                if i<self.checkpoint_nchunks: x,attn_wgt = checkpoint(blk, x)\n",
    "                else:                         x,attn_wgt = blk(x)\n",
    "                attn_wgts.append(attn_wgt)\n",
    "            return x,attn_wgts\n",
    "        \n",
    "        else:\n",
    "            for i,blk in enumerate(self.blocks):\n",
    "                if i<self.checkpoint_nchunks: x,_ = checkpoint(blk, x)\n",
    "                else:                         x,_ = blk(x)\n",
    "            return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.forward_features(x)\n",
    "    \n",
    "    \n",
    "class MultiCropViT(Module):\n",
    "    \"Multi Scale Multi Crop ViT Model\"\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 input_res=384, high_res=448, min_obj_area=64*64, crop_sz=128,\n",
    "                 crop_object=True, crop_object_parts=True,\n",
    "                 do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7,\n",
    "                 encoder_nblocks=12, checkpoint_nchunks=12):\n",
    "        \n",
    "        store_attr()\n",
    "\n",
    "        self.image_encoder = ViTEncoder(encoder, nblocks=encoder_nblocks, checkpoint_nchunks=checkpoint_nchunks)\n",
    "        self.norm = partial(nn.LayerNorm, eps=1e-6)(768)        \n",
    "        self.classifier = create_cls_module(768, 120, lin_ftrs=[768], use_bn=False, first_bn=False, ps=0.)\n",
    "    \n",
    "        \n",
    "    def forward(self, xb_high_res):\n",
    "\n",
    "        # get full image attention weigths / feature\n",
    "        self.image_encoder.return_attn_wgts = True\n",
    "        xb_input_res = F.interpolate(xb_high_res, size=(self.input_res,self.input_res))\n",
    "        _, attn_wgts = self.image_encoder(xb_input_res)\n",
    "        self.image_encoder.return_attn_wgts = False\n",
    "        \n",
    "        # get attention maps\n",
    "        attn_maps = generate_batch_attention_maps(attn_wgts, None, mode=None).detach()\n",
    "        attn_maps_high_res = interpolate_bil(attn_maps[None,...],self.high_res)[0]\n",
    "        attn_maps_input_res = interpolate_bil(attn_maps[None,...],self.input_res)[0]\n",
    "        \n",
    "\n",
    "        \n",
    "        #### ORIGINAL IMAGE ####\n",
    "        # original image attention erasing and features\n",
    "        if (self.training and self.do_attn_erasing):\n",
    "            xb_input_res = apply_attn_erasing(xb_input_res, attn_maps_input_res, self.attn_erasing_thresh, self.p_attn_erasing)\n",
    "        x_full = self.image_encoder(xb_input_res)\n",
    "\n",
    "        \n",
    "        \n",
    "        #### OBJECT CROP ####        \n",
    "        if self.crop_object:\n",
    "            # get object bboxes\n",
    "            batch_object_bboxes = np.vstack([generate_attention_coordinates(attn_map, \n",
    "                                                                            num_bboxes=1,\n",
    "                                                                            min_area=self.min_obj_area,\n",
    "                                                                            random_crop_sz=self.input_res)\n",
    "                                                    for attn_map in to_np(attn_maps_high_res)])\n",
    "            # crop objects\n",
    "            xb_objects, attn_maps_objects = [], []\n",
    "            for i, obj_bbox in enumerate(batch_object_bboxes):\n",
    "                minr, minc, maxr, maxc = obj_bbox\n",
    "                xb_objects        += [interpolate_bil(xb_high_res[i][:,minr:maxr,minc:maxc][None,...],self.input_res)[0]]\n",
    "                attn_maps_objects += [interpolate_bil(attn_maps_high_res[i][minr:maxr,minc:maxc][None,None,...],self.input_res)[0][0]]\n",
    "            xb_objects,attn_maps_objects = torch.stack(xb_objects),torch.stack(attn_maps_objects)\n",
    "\n",
    "            # object image attention erasing and features\n",
    "            if (self.training and self.do_attn_erasing):\n",
    "                xb_objects = apply_attn_erasing(xb_objects, attn_maps_objects, self.attn_erasing_thresh, self.p_attn_erasing)\n",
    "            x_object = self.image_encoder(xb_objects)\n",
    "                    \n",
    "        \n",
    "\n",
    "        #### OBJECT CROP PARTS ####\n",
    "        if self.crop_object_parts:\n",
    "            #get object crop bboxes\n",
    "            small_attn_maps_objects = interpolate_bil(attn_maps_objects[None,],self.input_res//3)[0] # to speed up calculation\n",
    "            batch_crop_bboxes = generate_batch_crops(small_attn_maps_objects.cpu(),\n",
    "                                                     source_sz=self.input_res//3, \n",
    "                                                     targ_sz=self.input_res, \n",
    "                                                     targ_bbox_sz=self.crop_sz,\n",
    "                                                     num_bboxes=2,\n",
    "                                                     nms_thresh=0.1)\n",
    "\n",
    "            # crop object parts\n",
    "            xb_crops1,xb_crops2 = [],[]\n",
    "            for i, crop_bboxes in enumerate(batch_crop_bboxes):\n",
    "                minr, minc, maxr, maxc = crop_bboxes[0]\n",
    "                xb_crops1 += [interpolate_bil(xb_objects[i][:,minr:maxr,minc:maxc][None,...],self.input_res)[0]]\n",
    "                minr, minc, maxr, maxc = crop_bboxes[1]\n",
    "                xb_crops2 += [interpolate_bil(xb_objects[i][:,minr:maxr,minc:maxc][None,...],self.input_res)[0]]\n",
    "            xb_crops1,xb_crops2 = torch.stack(xb_crops1),torch.stack(xb_crops2)\n",
    "\n",
    "            # crop features\n",
    "            x_crops1 = self.image_encoder(xb_crops1)\n",
    "            x_crops2 = self.image_encoder(xb_crops2)\n",
    "        \n",
    "        \n",
    "        # predict\n",
    "        x_full = self.norm(x_full)[:,0]\n",
    "        if self.crop_object:\n",
    "            x_object = self.norm(x_object)[:,0]\n",
    "            if self.crop_object_parts:\n",
    "                x_crops1 = self.norm(x_crops1)[:,0]\n",
    "                x_crops2 = self.norm(x_crops2)[:,0]\n",
    "                return self.classifier(x_full), self.classifier(x_object), self.classifier(x_crops1), self.classifier(x_crops2)\n",
    "            return self.classifier(x_full), self.classifier(x_object)\n",
    "        return  self.classifier(x_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_splitter(m): return L(m.image_encoder, m.norm, m.classifier).map(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFuncA(Module): # only object\n",
    "    def __init__(self):               self.lf = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "    def forward(self, preds, targs):  return self.lf(preds[1],targs)\n",
    "    \n",
    "class LossFuncB(Module): # full + object\n",
    "    def __init__(self):               self.lf = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "    def forward(self, preds, targs):  return self.lf(preds[0],targs) + self.lf(preds[1],targs)\n",
    "    \n",
    "class LossFuncC(Module): # full + object + crops\n",
    "    def __init__(self):               self.lf = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "    def forward(self, preds, targs):  return self.lf(preds[0],targs) + self.lf(preds[1],targs) + (self.lf(preds[2],targs)+self.lf(preds[3],targs))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyA(preds, targs): return accuracy(preds[1], targs)\n",
    "def accuracyB(preds, targs): return accuracy((preds[2]+preds[3])/2, targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1fhjl4sv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 278524<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a26c78ee614e4497fab12b2c8a5912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.02MB of 0.02MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(8):\n",
    "\n",
    "    if i == 0:\n",
    "        # exp 1 - full image\n",
    "        model_config = dict(crop_object=False, crop_object_parts=False, do_attn_erasing=False, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "        metrics =[accuracy] \n",
    "\n",
    "    if i == 1:\n",
    "        # exp 2 - full image\n",
    "        model_config = dict(crop_object=False, crop_object_parts=False, do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "        metrics =[accuracy] \n",
    "\n",
    "    if i == 2:\n",
    "        # exp 3 - object\n",
    "        model_config = dict(crop_object=True, crop_object_parts=False, do_attn_erasing=False, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncA()\n",
    "        metrics =[accuracyA] \n",
    "\n",
    "    if i == 3:\n",
    "        # exp 4 - object\n",
    "        model_config = dict(crop_object=True, crop_object_parts=False, do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncA()\n",
    "        metrics =[accuracyA] \n",
    "\n",
    "    if i == 4:\n",
    "\n",
    "        # exp 5 - full image + object\n",
    "        model_config = dict(crop_object=True, crop_object_parts=False, do_attn_erasing=False, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncB()\n",
    "        metrics =[accuracy, accuracyA] \n",
    "\n",
    "    if i == 5:\n",
    "        # exp 6 - full image + object\n",
    "        model_config = dict(crop_object=True, crop_object_parts=False, do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncB()\n",
    "        metrics =[accuracy, accuracyA]\n",
    "\n",
    "    if i == 6:\n",
    "        # exp 7 - full image + object + crops\n",
    "        model_config = dict(crop_object=True, crop_object_parts=True, do_attn_erasing=False, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncC()\n",
    "        metrics =[accuracy, accuracyA, accuracyB]\n",
    "\n",
    "    if i == 7:\n",
    "        # exp 8 - full image + object + crops\n",
    "        model_config = dict(crop_object=True, crop_object_parts=True, do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncC()\n",
    "        metrics =[accuracy, accuracyA, accuracyB]\n",
    "\n",
    "    # modified timm vit encoder\n",
    "    arch = \"vit_base_patch16_384\"\n",
    "    _encoder = create_encoder(arch, pretrained=True, n_in=3)\n",
    "    encoder = VisionTransformer(img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12)\n",
    "    encoder.head = Identity()\n",
    "    encoder.load_state_dict(_encoder.state_dict());\n",
    "\n",
    "    mcvit_model = MultiCropViT(encoder, input_res=384, high_res=448, min_obj_area=64*64, crop_sz=128,\n",
    "                                 encoder_nblocks=12, checkpoint_nchunks=12, **model_config)\n",
    "\n",
    "    WANDB = True\n",
    "    if WANDB:\n",
    "        xtra_config = model_config\n",
    "        xtra_config.update({\"Dataset\":\"Stanford Dogs\"})\n",
    "        wandb.init(project=\"fgvc-2021\", config=xtra_config);\n",
    "\n",
    "    cbs = []\n",
    "    if WANDB: cbs += [WandbCallback(log_preds=False,log_model=False)]\n",
    "    learn = Learner(dls, mcvit_model, opt_func=ranger, cbs=cbs, metrics=metrics, loss_func=loss_func, splitter=model_splitter)\n",
    "    learn.to_fp16();\n",
    "\n",
    "    epochs = 1\n",
    "    lr = 3e-3\n",
    "    learn.freeze_to(1)\n",
    "    learn.fit_one_cycle(epochs, lr_max=(lr), pct_start=0.5)\n",
    "\n",
    "    lr /= 3 \n",
    "    learn.unfreeze()\n",
    "    learn.fit_one_cycle(int(epochs**2), lr_max=[lr/3,lr,lr], pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
