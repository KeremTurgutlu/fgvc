{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from self_supervised.layers import *\n",
    "import sklearn\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_vit import *\n",
    "from utils.attention import *\n",
    "from utils.object_crops import *\n",
    "from utils.part_crops import *\n",
    "from utils.multi_crop_model import *\n",
    "\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path(\"../data/cub_200_2011/CUB_200_2011/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#9) ['classes.txt','bounding_boxes.txt','images.txt','README','images','attributes','parts','train_test_split.txt','image_class_labels.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath.ls().map(lambda o: o.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = get_image_files(datapath/'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11788"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('../data/cub_200_2011/CUB_200_2011/images/127.Savannah_Sparrow/Savannah_Sparrow_0059_119810.jpg'),Path('../data/cub_200_2011/CUB_200_2011/images/127.Savannah_Sparrow/Savannah_Sparrow_0067_118491.jpg'),Path('../data/cub_200_2011/CUB_200_2011/images/127.Savannah_Sparrow/Savannah_Sparrow_0118_118603.jpg')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_df = pd.read_csv(datapath/'train_test_split.txt', delimiter=' ', names=['image_id', 'is_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>is_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id  is_train\n",
       "0         1         0\n",
       "1         2         1\n",
       "2         3         0\n",
       "3         4         1\n",
       "4         5         1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5994\n",
       "0    5794\n",
       "Name: is_train, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split_df['is_train'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df = pd.read_csv(datapath/'images.txt', delimiter=' ', names=['image_id', 'filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>001.Black_footed_Albatross/Black_Footed_Albatross_0009_34.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>001.Black_footed_Albatross/Black_Footed_Albatross_0002_55.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>001.Black_footed_Albatross/Black_Footed_Albatross_0074_59.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>001.Black_footed_Albatross/Black_Footed_Albatross_0014_89.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                                                       filename\n",
       "0         1  001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
       "1         2  001.Black_footed_Albatross/Black_Footed_Albatross_0009_34.jpg\n",
       "2         3  001.Black_footed_Albatross/Black_Footed_Albatross_0002_55.jpg\n",
       "3         4  001.Black_footed_Albatross/Black_Footed_Albatross_0074_59.jpg\n",
       "4         5  001.Black_footed_Albatross/Black_Footed_Albatross_0014_89.jpg"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = images_df.merge(train_test_split_df, on='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn2istrain = dict(zip(merged_df['filename'], merged_df['is_train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('../data/cub_200_2011/CUB_200_2011/images/127.Savannah_Sparrow/Savannah_Sparrow_0059_119810.jpg')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = [datapath/'images'/o for o in merged_df.query(\"is_train == 1\")['filename'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11788"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(filename):      return PILImage.create(filename)\n",
    "def read_image_size(filename): return PILImage.create(filename).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label(filename): return filename.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sizes = parallel(read_image_size, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "size,bs = 448,20\n",
    "\n",
    "tfms = [[read_image, ToTensor, RandomResizedCrop(size, min_scale=.75)], \n",
    "        [read_label, Categorize()]]\n",
    "\n",
    "valid_splitter = lambda o: True if not fn2istrain[str(Path(o.parent.name)/o.name)] else False \n",
    "dsets = Datasets(filenames, tfms=tfms, splits=FuncSplitter(valid_splitter)(filenames))\n",
    "# dsets = Datasets(train_filenames, tfms=tfms, splits=None)\n",
    "\n",
    "\n",
    "batch_augs = aug_transforms()\n",
    "\n",
    "stats = imagenet_stats\n",
    "\n",
    "batch_tfms = [IntToFloatTensor] + batch_augs + [Normalize.from_stats(*stats)]\n",
    "dls = dsets.dataloaders(bs=bs, after_batch=batch_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5994, 5794)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dls.train_ds), len(dls.valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_bil(x,sz): return F.interpolate(x,mode='bilinear',align_corners=True, size=(sz,sz))\n",
    "\n",
    "def apply_attn_erasing(x, attn_maps, thresh, p=0.5): \n",
    "    \"x: bs x c x h x w, attn_maps: bs x h x w\"\n",
    "    erasing_mask = (attn_maps>thresh).unsqueeze(1)\n",
    "    ps = torch.zeros(erasing_mask.size(0)).float().bernoulli(p).to(erasing_mask.device)\n",
    "    rand_erasing_mask = 1-erasing_mask*ps[...,None,None,None]\n",
    "    return rand_erasing_mask*x\n",
    "\n",
    "class ViTEncoder(Module):\n",
    "    \"Timm ViT encoder which return encoder outputs and optionally returns attention weights with gradient checkpointing\"\n",
    "    def __init__(self, vit, nblocks=12, checkpoint_nchunks=2, return_attn_wgts=True):\n",
    "                \n",
    "        # initialize params\n",
    "        self.patch_embed = vit.patch_embed\n",
    "        self.cls_token = vit.cls_token\n",
    "        self.pos_embed = vit.pos_embed\n",
    "        self.pos_drop = vit.pos_drop\n",
    "        \n",
    "        # until any desired layers\n",
    "        self.blocks = vit.blocks[:nblocks]        \n",
    "        \n",
    "        # gradient checkpointing\n",
    "        self.checkpoint_nchunks = checkpoint_nchunks\n",
    "        \n",
    "        # return attention weights from L layers\n",
    "        self.return_attn_wgts = return_attn_wgts\n",
    "         \n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "\n",
    "        # collect attn_wgts from all layers\n",
    "        if self.return_attn_wgts:\n",
    "            attn_wgts = []\n",
    "            for i,blk in enumerate(self.blocks):\n",
    "                if i<self.checkpoint_nchunks: x,attn_wgt = checkpoint(blk, x)\n",
    "                else:                         x,attn_wgt = blk(x)\n",
    "                attn_wgts.append(attn_wgt)\n",
    "            return x,attn_wgts\n",
    "        \n",
    "        else:\n",
    "            for i,blk in enumerate(self.blocks):\n",
    "                if i<self.checkpoint_nchunks: x,_ = checkpoint(blk, x)\n",
    "                else:                         x,_ = blk(x)\n",
    "            return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.forward_features(x)\n",
    "    \n",
    "    \n",
    "class MultiCropViT(Module):\n",
    "    \"Multi Scale Multi Crop ViT Model\"\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 num_classes,\n",
    "                 input_res=384, high_res=786, min_obj_area=112*112, crop_sz=224,\n",
    "                 crop_object=True, crop_object_parts=True,\n",
    "                 do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7,\n",
    "                 encoder_nblocks=12, checkpoint_nchunks=12):\n",
    "        \n",
    "        store_attr()\n",
    "\n",
    "        self.image_encoder = ViTEncoder(encoder, nblocks=encoder_nblocks, checkpoint_nchunks=checkpoint_nchunks)\n",
    "        self.norm = partial(nn.LayerNorm, eps=1e-6)(768)        \n",
    "        self.classifier = create_cls_module(768, num_classes, lin_ftrs=[768], use_bn=False, first_bn=False, ps=0.)\n",
    "    \n",
    "        \n",
    "    def forward(self, xb_high_res):\n",
    "\n",
    "        # get full image attention weigths / feature\n",
    "        self.image_encoder.return_attn_wgts = True\n",
    "        xb_input_res = F.interpolate(xb_high_res, size=(self.input_res,self.input_res))\n",
    "        _, attn_wgts = self.image_encoder(xb_input_res)\n",
    "        self.image_encoder.return_attn_wgts = False\n",
    "        \n",
    "        # get attention maps\n",
    "        attn_maps = generate_batch_attention_maps(attn_wgts, None, mode=None).detach()\n",
    "        attn_maps_high_res = interpolate_bil(attn_maps[None,...],self.high_res)[0]\n",
    "        attn_maps_input_res = interpolate_bil(attn_maps[None,...],self.input_res)[0]\n",
    "        \n",
    "\n",
    "        \n",
    "        #### ORIGINAL IMAGE ####\n",
    "        # original image attention erasing and features\n",
    "        if (self.training and self.do_attn_erasing):\n",
    "            xb_input_res = apply_attn_erasing(xb_input_res, attn_maps_input_res, self.attn_erasing_thresh, self.p_attn_erasing)\n",
    "        x_full = self.image_encoder(xb_input_res)\n",
    "\n",
    "        \n",
    "        \n",
    "        #### OBJECT CROP ####        \n",
    "        if self.crop_object:\n",
    "            # get object bboxes\n",
    "            batch_object_bboxes = np.vstack([generate_attention_coordinates(attn_map, \n",
    "                                                                            num_bboxes=1,\n",
    "                                                                            min_area=self.min_obj_area,\n",
    "                                                                            random_crop_sz=self.input_res)\n",
    "                                                    for attn_map in to_np(attn_maps_high_res)])\n",
    "            # crop objects\n",
    "            xb_objects, attn_maps_objects = [], []\n",
    "            for i, obj_bbox in enumerate(batch_object_bboxes):\n",
    "                minr, minc, maxr, maxc = obj_bbox\n",
    "                xb_objects        += [interpolate_bil(xb_high_res[i][:,minr:maxr,minc:maxc][None,...],self.input_res)[0]]\n",
    "                attn_maps_objects += [interpolate_bil(attn_maps_high_res[i][minr:maxr,minc:maxc][None,None,...],self.input_res)[0][0]]\n",
    "            xb_objects,attn_maps_objects = torch.stack(xb_objects),torch.stack(attn_maps_objects)\n",
    "\n",
    "            # object image attention erasing and features\n",
    "            if (self.training and self.do_attn_erasing):\n",
    "                xb_objects = apply_attn_erasing(xb_objects, attn_maps_objects, self.attn_erasing_thresh, self.p_attn_erasing)\n",
    "            x_object = self.image_encoder(xb_objects)\n",
    "                    \n",
    "        \n",
    "\n",
    "        #### OBJECT CROP PARTS ####\n",
    "        if self.crop_object_parts:\n",
    "            #get object crop bboxes\n",
    "            small_attn_maps_objects = interpolate_bil(attn_maps_objects[None,],self.input_res//3)[0] # to speed up calculation\n",
    "            batch_crop_bboxes = generate_batch_crops(small_attn_maps_objects.cpu(),\n",
    "                                                     source_sz=self.input_res//3, \n",
    "                                                     targ_sz=self.input_res, \n",
    "                                                     targ_bbox_sz=self.crop_sz,\n",
    "                                                     num_bboxes=2,\n",
    "                                                     nms_thresh=0.1)\n",
    "\n",
    "            # crop object parts\n",
    "            xb_crops1,xb_crops2 = [],[]\n",
    "            for i, crop_bboxes in enumerate(batch_crop_bboxes):\n",
    "                minr, minc, maxr, maxc = crop_bboxes[0]\n",
    "                xb_crops1 += [interpolate_bil(xb_objects[i][:,minr:maxr,minc:maxc][None,...],self.input_res)[0]]\n",
    "                minr, minc, maxr, maxc = crop_bboxes[1]\n",
    "                xb_crops2 += [interpolate_bil(xb_objects[i][:,minr:maxr,minc:maxc][None,...],self.input_res)[0]]\n",
    "            xb_crops1,xb_crops2 = torch.stack(xb_crops1),torch.stack(xb_crops2)\n",
    "\n",
    "            # crop features\n",
    "            x_crops1 = self.image_encoder(xb_crops1)\n",
    "            x_crops2 = self.image_encoder(xb_crops2)\n",
    "        \n",
    "        \n",
    "        # predict\n",
    "        x_full = self.norm(x_full)[:,0]\n",
    "        if self.crop_object:\n",
    "            x_object = self.norm(x_object)[:,0]\n",
    "            if self.crop_object_parts:\n",
    "                x_crops1 = self.norm(x_crops1)[:,0]\n",
    "                x_crops2 = self.norm(x_crops2)[:,0]\n",
    "                return self.classifier(x_full), self.classifier(x_object), self.classifier(x_crops1), self.classifier(x_crops2)\n",
    "            return self.classifier(x_full), self.classifier(x_object)\n",
    "        return  self.classifier(x_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_splitter(m): return L(m.image_encoder, m.norm, m.classifier).map(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFuncA(Module): # only object\n",
    "    def __init__(self):               self.lf = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "    def forward(self, preds, targs):  return self.lf(preds[1],targs)\n",
    "    \n",
    "class LossFuncB(Module): # full + object\n",
    "    def __init__(self):               self.lf = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "    def forward(self, preds, targs):  return self.lf(preds[0],targs) + self.lf(preds[1],targs)\n",
    "    \n",
    "class LossFuncC(Module): # full + object + crops\n",
    "    def __init__(self):               self.lf = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "    def forward(self, preds, targs):  return self.lf(preds[0],targs) + self.lf(preds[1],targs) + (self.lf(preds[2],targs)+self.lf(preds[3],targs))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyA(preds, targs): return accuracy(preds[0], targs) # full\n",
    "def accuracyB(preds, targs): return accuracy(preds[1], targs) # full, object\n",
    "def accuracyC(preds, targs): return accuracy((preds[2]+preds[3])/2, targs) # full, object, crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkeremturgutlu\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.25 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">noble-pond-94</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/keremturgutlu/fgvc-2021\" target=\"_blank\">https://wandb.ai/keremturgutlu/fgvc-2021</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/keremturgutlu/fgvc-2021/runs/26zo787r\" target=\"_blank\">https://wandb.ai/keremturgutlu/fgvc-2021/runs/26zo787r</a><br/>\n",
       "                Run data is saved locally in <code>/home/code-base/fgvc/stanford_dog/wandb/run-20210409_022746-26zo787r</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracyA</th>\n",
       "      <th>accuracyB</th>\n",
       "      <th>accuracyC</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.676136</td>\n",
       "      <td>6.286286</td>\n",
       "      <td>0.649465</td>\n",
       "      <td>0.641698</td>\n",
       "      <td>0.566966</td>\n",
       "      <td>19:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.719569</td>\n",
       "      <td>4.658880</td>\n",
       "      <td>0.854850</td>\n",
       "      <td>0.857439</td>\n",
       "      <td>0.807214</td>\n",
       "      <td>19:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2' class='' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      50.00% [2/4 58:06<58:06]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracyA</th>\n",
       "      <th>accuracyB</th>\n",
       "      <th>accuracyC</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.044307</td>\n",
       "      <td>4.320858</td>\n",
       "      <td>0.871764</td>\n",
       "      <td>0.878668</td>\n",
       "      <td>0.846911</td>\n",
       "      <td>29:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.947448</td>\n",
       "      <td>4.185483</td>\n",
       "      <td>0.870038</td>\n",
       "      <td>0.876942</td>\n",
       "      <td>0.850362</td>\n",
       "      <td>29:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='31' class='' max='299' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      10.37% [31/299 02:00<17:23 3.7864]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [6]:\n",
    "\n",
    "    if i == 0:\n",
    "        # exp 1 - full image\n",
    "        model_config = dict(crop_object=False, crop_object_parts=False, do_attn_erasing=False, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "        metrics =[accuracy] \n",
    "\n",
    "    if i == 1:\n",
    "        # exp 2 - full image\n",
    "        model_config = dict(crop_object=False, crop_object_parts=False, do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LabelSmoothingCrossEntropyFlat(0.1)\n",
    "        metrics =[accuracy] \n",
    "\n",
    "    if i == 2:\n",
    "        # exp 3 - object\n",
    "        model_config = dict(crop_object=True, crop_object_parts=False, do_attn_erasing=False, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncA()\n",
    "        metrics =[accuracyB] \n",
    "\n",
    "    if i == 3:\n",
    "        # exp 4 - object\n",
    "        model_config = dict(crop_object=True, crop_object_parts=False, do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncA()\n",
    "        metrics =[accuracyB] \n",
    "\n",
    "    if i == 4:\n",
    "        # exp 5 - full image + object\n",
    "        model_config = dict(crop_object=True, crop_object_parts=False, do_attn_erasing=False, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncB()\n",
    "        metrics =[accuracyA, accuracyB] \n",
    "\n",
    "    if i == 5:\n",
    "        # exp 6 - full image + object\n",
    "        model_config = dict(crop_object=True, crop_object_parts=False, do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncB()\n",
    "        metrics =[accuracyA, accuracyB]\n",
    "\n",
    "    if i == 6:\n",
    "        # exp 7 - full image + object + crops\n",
    "        model_config = dict(crop_object=True, crop_object_parts=True, do_attn_erasing=False, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncC()\n",
    "        metrics =[accuracyA, accuracyB, accuracyC]\n",
    "\n",
    "    if i == 7:\n",
    "        # exp 8 - full image + object + crops\n",
    "        model_config = dict(crop_object=True, crop_object_parts=True, do_attn_erasing=True, p_attn_erasing=0.5, attn_erasing_thresh=0.7)\n",
    "        loss_func = LossFuncC()\n",
    "        metrics =[accuracyA, accuracyB, accuracyC]\n",
    "\n",
    "    # modified timm vit encoder\n",
    "    arch = \"vit_base_patch16_384\"\n",
    "    _encoder = create_encoder(arch, pretrained=True, n_in=3)\n",
    "    encoder = VisionTransformer(img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12)\n",
    "    encoder.head = Identity()\n",
    "    encoder.load_state_dict(_encoder.state_dict());\n",
    "    \n",
    "    \n",
    "    high_res=size\n",
    "    min_obj_area=64*64\n",
    "    crop_sz=128\n",
    "    \n",
    "\n",
    "    mcvit_model = MultiCropViT(encoder, num_classes=dls.c, input_res=384, high_res=high_res, min_obj_area=min_obj_area, crop_sz=crop_sz,\n",
    "                                 encoder_nblocks=12, checkpoint_nchunks=12, **model_config)\n",
    "\n",
    "    WANDB = True\n",
    "    if WANDB:\n",
    "        xtra_config = model_config\n",
    "        xtra_config.update({\"Dataset\":\"CUB-200-2011\"})\n",
    "        wandb.init(project=\"fgvc-2021\", config=xtra_config);\n",
    "\n",
    "    cbs = []\n",
    "    if WANDB: cbs += [WandbCallback(log_preds=False,log_model=False), SaveModelCallback(every_epoch=True, fname=\"cup_model_epoch\")]\n",
    "    learn = Learner(dls, mcvit_model, opt_func=ranger, cbs=cbs, metrics=metrics, loss_func=loss_func, splitter=model_splitter)\n",
    "    learn.to_fp16();\n",
    "    \n",
    "                \n",
    "    lr = 3e-3\n",
    "    epochs = 2\n",
    "    learn.freeze_to(-1)\n",
    "    learn.fit_one_cycle(epochs, lr_max=(lr), pct_start=0.5)\n",
    "\n",
    "    lr /= 3\n",
    "    epochs = 4\n",
    "    learn.unfreeze()\n",
    "    learn.fit_one_cycle(int(epochs), lr_max=[lr/50,lr,lr], pct_start=0.5)\n",
    "\n",
    "    del learn, encoder, mcvit_model\n",
    "    gc.collect()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
