{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from self_supervised.layers import *\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path(\"../data/stanford-dogs-dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(datapath/'train.csv')\n",
    "test_df = pd.read_csv(datapath/'test.csv')\n",
    "sample_df = pd.read_csv(datapath/'sample_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12000, 2), (8580, 2), (6000, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape, sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head()\n",
    "# test_df.head()\n",
    "# sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(filename): return PILImage.create(datapath/'images/Images'/filename)\n",
    "def read_image_size(filename): return PILImage.create(datapath/'images/Images'/filename).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FAST:\n",
    "    filenames = sample_df['filename'].values\n",
    "    labels = sample_df['label'].values\n",
    "    fn2label = dict(zip(filenames, labels))\n",
    "else:\n",
    "    filenames = train_df['filenames'].values\n",
    "    labels = train_df['labels'].values\n",
    "    fn2label = dict(zip(filenames, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label(filename): return fn2label[filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_filenames = sample_df.query(\"split == 'valid'\")['filename'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size,bs = 384,32\n",
    "\n",
    "tfms = [[read_image, ToTensor, RandomResizedCrop(size, min_scale=.75)], \n",
    "        [read_label, Categorize()]]\n",
    "\n",
    "valid_splitter = lambda o: True if o in valid_filenames else False \n",
    "dsets = Datasets(filenames, tfms=tfms, splits=FuncSplitter(valid_splitter)(filenames))\n",
    "\n",
    "batch_augs = aug_transforms()\n",
    "# batch_augs = []\n",
    "\n",
    "stats = imagenet_stats\n",
    "\n",
    "batch_tfms = [IntToFloatTensor] + batch_augs + [Normalize.from_stats(*stats)]\n",
    "dls = dsets.dataloaders(bs=bs, after_batch=batch_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4800, 1200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dls.train_ds), len(dls.valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifications on ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_vit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timm vit _encoder\n",
    "arch = \"vit_base_patch16_384\"\n",
    "_encoder = create_encoder(arch, pretrained=True, n_in=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom vit encoder with timm weights\n",
    "encoder = VisionTransformer(img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12)\n",
    "encoder.head = Identity()\n",
    "encoder.load_state_dict(_encoder.state_dict());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad checkpointing\n",
    "encoder = CheckpointVisionTransformer(encoder, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Change Stride Size\n",
    "\n",
    "patch_size,stride_size = 16,16\n",
    "\n",
    "# new_patch_embed = PatchEmbed(size, patch_size, stride_size)\n",
    "# new_patch_embed.proj.weight.data = encoder.vit_model.patch_embed.proj.weight.data\n",
    "# new_patch_embed.proj.bias.data = encoder.vit_model.patch_embed.proj.bias.data\n",
    "# encoder.vit_model.patch_embed = new_patch_embed\n",
    "\n",
    "# 2) Interpolate Position Embeddings to new Number of Patches\n",
    "\n",
    "# num_patches = ((size - patch_size + stride_size) // stride_size)**2 + 1\n",
    "\n",
    "# pos_embed_data = encoder.vit_model.pos_embed.data\n",
    "# new_pos_embed_data = F.interpolate(pos_embed_data[None, ...], \n",
    "#                                    size=[num_patches, pos_embed_data.size(-1)], \n",
    "#                                    mode='nearest')[0]\n",
    "# encoder.vit_model.pos_embed.data = new_pos_embed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out, attn_wgts = encoder(torch.randn(2,3,size,size))\n",
    "    nf = out.size(1)\n",
    "classifier = create_cls_module(nf, dls.c, lin_ftrs=[768], use_bn=False, first_bn=False, ps=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGVCModel(Module):\n",
    "    def __init__(self, encoder, classifier, return_attn_wgts=False):\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "        self.return_attn_wgts = return_attn_wgts\n",
    "        \n",
    "    def forward(self, x):\n",
    "        cls_token,attn_wgts = self.encoder(x)\n",
    "        if self.return_attn_wgts: return self.classifier(cls_token), attn_wgts, cls_token\n",
    "        else:                     return self.classifier(cls_token)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FGVCModel(encoder, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 577, 577])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_wgts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Linear(in_features=768, out_features=120, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_splitter(m): return L(m[0], m[1]).map(params)\n",
    "def model_splitter(m): return L(m.encoder, m.classifier).map(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = []\n",
    "# if WANDB: cbs += [WandbCallback(log_preds=False,log_model=False)]\n",
    "learn = Learner(dls, model, opt_func=ranger, cbs=cbs, metrics=[accuracy], splitter=model_splitter,\n",
    "                loss_func=LabelSmoothingCrossEntropyFlat(0.1))\n",
    "learn.to_fp16();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 2\n",
    "\n",
    "# lr = 3e-3\n",
    "# learn.freeze()\n",
    "# learn.fit_one_cycle(epochs, lr_max=(lr), pct_start=0.5)\n",
    "\n",
    "# lr /= 3 \n",
    "# learn.unfreeze()\n",
    "# learn.fit_one_cycle(int(epochs**2), lr_max=slice(lr/100, lr), pct_start=0.5)\n",
    "\n",
    "# learn.save(f\"{arch}_stride_{stride_size}_imsize_{size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f\"{arch}_stride_{stride_size}_imsize_384\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [1.0573927164077759,0.9158333539962769]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Crop Dataset : 1 x (384 px whole image) + 2 x (448 px -> 112 px crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.attention import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "size,bs = 448,16\n",
    "\n",
    "tfms = [[read_image, ToTensor, RandomResizedCrop(size, min_scale=.75)], \n",
    "        [read_label, Categorize()]]\n",
    "\n",
    "valid_splitter = lambda o: True if o in valid_filenames else False \n",
    "dsets = Datasets(filenames, tfms=tfms, splits=FuncSplitter(valid_splitter)(filenames))\n",
    "\n",
    "batch_augs = aug_transforms()\n",
    "# batch_augs = []\n",
    "\n",
    "stats = imagenet_stats\n",
    "\n",
    "batch_tfms = [IntToFloatTensor] + batch_augs + [Normalize.from_stats(*stats)]\n",
    "dls = dsets.dataloaders(bs=bs, after_batch=batch_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vit_encoder = learn.model.encoder.vit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb_448,yb = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb_384 = F.interpolate(xb_448, size=(384,384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 448, 448]), torch.Size([16, 3, 384, 384]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb_448.shape, xb_384.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint\n",
    "class FullImageEncoder(Module):\n",
    "    \"Encoder which takes whole image input then outputs attention weights + layer features\"\n",
    "    def __init__(self, pretrained_vit_encoder, nblocks=11, checkpoint_nchunks=2, return_attn_wgts=True):\n",
    "                \n",
    "        # initialize params with warm up model\n",
    "        self.patch_embed = pretrained_vit_encoder.patch_embed\n",
    "        self.cls_token = pretrained_vit_encoder.cls_token\n",
    "        self.pos_embed = pretrained_vit_encoder.pos_embed\n",
    "        self.pos_drop = pretrained_vit_encoder.pos_drop\n",
    "        \n",
    "        # until layer n-1, can be changed (memory trade-off)\n",
    "        self.blocks = pretrained_vit_encoder.blocks[:nblocks]        \n",
    "        \n",
    "        # not needed now\n",
    "#         self.norm = pretrained_vit_encoder.norm\n",
    "        \n",
    "        # gradient checkpointing\n",
    "        self.checkpoint_nchunks = checkpoint_nchunks\n",
    "        \n",
    "        self.return_attn_wgts = return_attn_wgts\n",
    "         \n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # collect attn_wgts from all layers\n",
    "        attn_wgts = []\n",
    "        if self.return_attn_wgts:\n",
    "            for i,blk in enumerate(self.blocks):\n",
    "                if i<self.checkpoint_nchunks: x,attn_wgt = checkpoint(blk, x)\n",
    "                else:                         x,attn_wgt = blk(x)\n",
    "                attn_wgts.append(attn_wgt)\n",
    "            return x,attn_wgts\n",
    "        \n",
    "        else:\n",
    "            for i,blk in enumerate(self.blocks):\n",
    "                if i<self.checkpoint_nchunks: x,_ = checkpoint(blk, x)\n",
    "                else:                         x,_ = blk(x)\n",
    "            return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.forward_features(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vit_encoder = learn.model.encoder.vit_model\n",
    "full_image_encoder = FullImageEncoder(pretrained_vit_encoder, nblocks=11, checkpoint_nchunks=2).cuda() # init full iamge encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, attn_wgts = full_image_encoder(xb_384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 577, 768]), 11)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, len(attn_wgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_attention_maps(attn_wgts, targ_sz=None, mode=None):\n",
    "    \"Generate attention flow maps with shape (targ_sz,targ_sz) from L layer attetion weights of transformer model\"\n",
    "    # Stack for all layers - BS x L x K x gx x gy\n",
    "    att_mat = torch.stack(attn_wgts, dim=1)\n",
    "    # Average the attention weights across all heads.\n",
    "    att_mat = torch.mean(att_mat, dim=2)\n",
    "    # To account for residual connections, we add an identity matrix to the\n",
    "    aug_att_mat = att_mat + torch.eye(att_mat.size(-1))[None,None,...].to(att_mat.device)\n",
    "    # Re-normalize the weights.\n",
    "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "    # Recursively multiply the weight matrices\n",
    "    joint_attentions = aug_att_mat[:,0]\n",
    "    for n in range(1, aug_att_mat.size(1)): joint_attentions = torch.bmm(aug_att_mat[:,n], joint_attentions)\n",
    "\n",
    "    # BS x (num_patches+1) -> BS x gx x gy\n",
    "    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "    joint_attentions = joint_attentions[:,0,1:].view(joint_attentions.size(0),grid_size,grid_size)\n",
    "    joint_attentions /= torch.amax(joint_attentions, dim=(-2,-1), keepdim=True)\n",
    "\n",
    "    # Bilinear interpolation to target size\n",
    "    if mode == 'bilinear':\n",
    "        joint_attentions = F.interpolate(joint_attentions[None,...], \n",
    "                                         (targ_sz,targ_sz), \n",
    "                                         mode=mode, align_corners=True)[0].detach().cpu().numpy()\n",
    "    elif mode == 'nearest':\n",
    "        joint_attentions = F.interpolate(joint_attentions[None,...], \n",
    "                                         (targ_sz,targ_sz), \n",
    "                                         mode=mode)[0].detach().cpu().numpy()\n",
    "    elif mode is None:\n",
    "        joint_attentions = joint_attentions\n",
    "    \n",
    "    return joint_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_maps = generate_batch_attention_maps(attn_wgts, None, mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 24, 24])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_maps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialTransfomerBlock(Module):\n",
    "    def __init__(self):\n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = nn.Sequential(\n",
    "                            nn.MaxPool2d(2, stride=3),\n",
    "        )\n",
    "        \n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 2*4)\n",
    "        )\n",
    "        \n",
    "        # ranges for each geometrc transform\n",
    "        self.scale_range = SigmoidRange(0,1) # between 0-1 we always crop\n",
    "        self.translate_range = SigmoidRange(-1,1) # stay inside image\n",
    "        \n",
    "    \n",
    "    # Spatial transformer network forward function\n",
    "    def forward(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(xs.size(0), -1)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(x.size(0),2,4)\n",
    "        theta = torch.cat([self.scale_range(theta[:,:,:2]), self.translate_range(theta[:,:,2:])], dim=-1)\n",
    "\n",
    "        # scalex,scaley,translatex,translatey -> affine matrix\n",
    "        # [scalex,      0,   scalex*translatex]\n",
    "        # [0     , scaley,   scaley*translatey]\n",
    "        zeros = torch.zeros(theta.size(0),theta.size(1)).to(theta.device)\n",
    "        theta = torch.stack([theta[:,:,0],\n",
    "                             zeros,\n",
    "                             theta[:,:,0]*theta[:,:,2],\n",
    "                             zeros,\n",
    "                             theta[:,:,1],\n",
    "                             theta[:,:,1]*theta[:,:,3]], dim=-1).view(theta.size(0), theta.size(1), 2, 3)\n",
    "\n",
    "        return theta # BS x 2 (num_crops) x 2 x 3\n",
    "        \n",
    "    def transform(self, x, theta, targ_sz=112):\n",
    "        grid = F.affine_grid(theta, (x.size(0),x.size(1),targ_sz,targ_sz))\n",
    "        out = F.grid_sample(x, grid)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model = SpatialTransfomerBlock().cuda()\n",
    "theta_crops = st_model(attention_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del attn_wgts\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.5404,  0.0000, -0.0303],\n",
       "         [ 0.0000,  0.4677,  0.0425]], device='cuda:0',\n",
       "        grad_fn=<SelectBackward>),\n",
       " tensor([[ 0.4858,  0.0000, -0.0023],\n",
       "         [ 0.0000,  0.4988,  0.0113]], device='cuda:0',\n",
       "        grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_crops[0,0], theta_crops[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb_112_crop1 = st_model.transform(xb_448, theta_crops[:,0], targ_sz=112)\n",
    "xb_112_crop2 = st_model.transform(xb_448, theta_crops[:,0], targ_sz=112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 112, 112]), torch.Size([16, 3, 112, 112]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb_112_crop1.shape, xb_112_crop2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_image_encoder = deepcopy(full_image_encoder).cuda() # init crop encoder as copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_image_encoder.return_attn_wgts = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate pos embed from 384 px -> 112 px\n",
    "num_patches = ((112 - 16 + 16) // 16)**2 + 1\n",
    "pos_embed_data = crop_image_encoder.pos_embed.data\n",
    "new_pos_embed_data = F.interpolate(pos_embed_data[None, ...], \n",
    "                                   size=[num_patches, pos_embed_data.size(-1)], \n",
    "                                   mode='nearest')[0]\n",
    "crop_image_encoder.pos_embed.data = new_pos_embed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_crop1 = crop_image_encoder(xb_112_crop1)\n",
    "x_crop2 = crop_image_encoder(xb_112_crop1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 577, 768]),\n",
       " torch.Size([16, 50, 768]),\n",
       " torch.Size([16, 50, 768]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x_crop1.shape, x_crop2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat([x, x_crop1, x_crop2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 677, 768])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_block = Block(dim=768,num_heads=12,mlp_ratio=4.,qkv_bias=True,qk_scale=None).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,_ = final_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = partial(nn.LayerNorm, eps=1e-6)(768).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token = x[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = create_cls_module(nf, dls.c, lin_ftrs=[768], use_bn=False, first_bn=False, ps=0.).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 120])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(cls_token).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STViT(Module):\n",
    "    def __init__(self, pretrained_vit_encoder):\n",
    "        \n",
    "        \n",
    "        self.full_image_encoder = FullImageEncoder(pretrained_vit_encoder, nblocks=11, checkpoint_nchunks=6)\n",
    "        self.st_model = SpatialTransfomerBlock()\n",
    "        self.crop_image_encoder = deepcopy(full_image_encoder)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
