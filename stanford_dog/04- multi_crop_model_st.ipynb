{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from self_supervised.layers import *\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path(\"../data/stanford-dogs-dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(datapath/'train.csv')\n",
    "test_df = pd.read_csv(datapath/'test.csv')\n",
    "sample_df = pd.read_csv(datapath/'sample_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12000, 2), (8580, 2), (6000, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape, sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head()\n",
    "# test_df.head()\n",
    "# sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(filename): return PILImage.create(datapath/'images/Images'/filename)\n",
    "def read_image_size(filename): return PILImage.create(datapath/'images/Images'/filename).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FAST:\n",
    "    filenames = sample_df['filename'].values\n",
    "    labels = sample_df['label'].values\n",
    "    fn2label = dict(zip(filenames, labels))\n",
    "else:\n",
    "    filenames = train_df['filenames'].values\n",
    "    labels = train_df['labels'].values\n",
    "    fn2label = dict(zip(filenames, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label(filename): return fn2label[filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_filenames = sample_df.query(\"split == 'valid'\")['filename'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size,bs = 384,32\n",
    "\n",
    "tfms = [[read_image, ToTensor, RandomResizedCrop(size, min_scale=.75)], \n",
    "        [read_label, Categorize()]]\n",
    "\n",
    "valid_splitter = lambda o: True if o in valid_filenames else False \n",
    "dsets = Datasets(filenames, tfms=tfms, splits=FuncSplitter(valid_splitter)(filenames))\n",
    "\n",
    "batch_augs = aug_transforms()\n",
    "# batch_augs = []\n",
    "\n",
    "stats = imagenet_stats\n",
    "\n",
    "batch_tfms = [IntToFloatTensor] + batch_augs + [Normalize.from_stats(*stats)]\n",
    "dls = dsets.dataloaders(bs=bs, after_batch=batch_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4800, 1200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dls.train_ds), len(dls.valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifications on ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_vit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timm vit _encoder\n",
    "arch = \"vit_base_patch16_384\"\n",
    "_encoder = create_encoder(arch, pretrained=True, n_in=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom vit encoder with timm weights\n",
    "encoder = VisionTransformer(img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12)\n",
    "encoder.head = Identity()\n",
    "encoder.load_state_dict(_encoder.state_dict());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad checkpointing\n",
    "encoder = CheckpointVisionTransformer(encoder, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Change Stride Size\n",
    "\n",
    "patch_size,stride_size = 16,16\n",
    "\n",
    "# new_patch_embed = PatchEmbed(size, patch_size, stride_size)\n",
    "# new_patch_embed.proj.weight.data = encoder.vit_model.patch_embed.proj.weight.data\n",
    "# new_patch_embed.proj.bias.data = encoder.vit_model.patch_embed.proj.bias.data\n",
    "# encoder.vit_model.patch_embed = new_patch_embed\n",
    "\n",
    "# 2) Interpolate Position Embeddings to new Number of Patches\n",
    "\n",
    "# num_patches = ((size - patch_size + stride_size) // stride_size)**2 + 1\n",
    "\n",
    "# pos_embed_data = encoder.vit_model.pos_embed.data\n",
    "# new_pos_embed_data = F.interpolate(pos_embed_data[None, ...], \n",
    "#                                    size=[num_patches, pos_embed_data.size(-1)], \n",
    "#                                    mode='nearest')[0]\n",
    "# encoder.vit_model.pos_embed.data = new_pos_embed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out, attn_wgts = encoder(torch.randn(2,3,size,size))\n",
    "    nf = out.size(1)\n",
    "classifier = create_cls_module(nf, dls.c, lin_ftrs=[768], use_bn=False, first_bn=False, ps=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGVCModel(Module):\n",
    "    def __init__(self, encoder, classifier, return_attn_wgts=False):\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "        self.return_attn_wgts = return_attn_wgts\n",
    "        \n",
    "    def forward(self, x):\n",
    "        cls_token,attn_wgts = self.encoder(x)\n",
    "        if self.return_attn_wgts: return self.classifier(cls_token), attn_wgts, cls_token\n",
    "        else:                     return self.classifier(cls_token)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FGVCModel(encoder, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 577, 577])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_wgts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Linear(in_features=768, out_features=120, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_splitter(m): return L(m[0], m[1]).map(params)\n",
    "def model_splitter(m): return L(m.encoder, m.classifier).map(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = []\n",
    "# if WANDB: cbs += [WandbCallback(log_preds=False,log_model=False)]\n",
    "learn = Learner(dls, model, opt_func=ranger, cbs=cbs, metrics=[accuracy], splitter=model_splitter,\n",
    "                loss_func=LabelSmoothingCrossEntropyFlat(0.1))\n",
    "learn.to_fp16();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 2\n",
    "\n",
    "# lr = 3e-3\n",
    "# learn.freeze()\n",
    "# learn.fit_one_cycle(epochs, lr_max=(lr), pct_start=0.5)\n",
    "\n",
    "# lr /= 3 \n",
    "# learn.unfreeze()\n",
    "# learn.fit_one_cycle(int(epochs**2), lr_max=slice(lr/100, lr), pct_start=0.5)\n",
    "\n",
    "# learn.save(f\"{arch}_stride_{stride_size}_imsize_{size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f\"{arch}_stride_{stride_size}_imsize_384\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vit_encoder = learn.model.encoder.vit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del learn, dls\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Crop Dataset : 1 x (384 px whole image) + 2 x (448 px -> 112 px crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.attention import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "size,bs = 448,16\n",
    "\n",
    "tfms = [[read_image, ToTensor, RandomResizedCrop(size, min_scale=.75)], \n",
    "        [read_label, Categorize()]]\n",
    "\n",
    "valid_splitter = lambda o: True if o in valid_filenames else False \n",
    "dsets = Datasets(filenames, tfms=tfms, splits=FuncSplitter(valid_splitter)(filenames))\n",
    "\n",
    "batch_augs = aug_transforms()\n",
    "# batch_augs = []\n",
    "\n",
    "stats = imagenet_stats\n",
    "\n",
    "batch_tfms = [IntToFloatTensor] + batch_augs + [Normalize.from_stats(*stats)]\n",
    "dls = dsets.dataloaders(bs=bs, after_batch=batch_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint\n",
    "class FullImageEncoder(Module):\n",
    "    \"Encoder which takes whole image input then outputs attention weights + layer features\"\n",
    "    def __init__(self, pretrained_vit_encoder, nblocks=11, checkpoint_nchunks=2, return_attn_wgts=True):\n",
    "                \n",
    "        # initialize params with warm up model\n",
    "        self.patch_embed = pretrained_vit_encoder.patch_embed\n",
    "        self.cls_token = pretrained_vit_encoder.cls_token\n",
    "        self.pos_embed = pretrained_vit_encoder.pos_embed\n",
    "        self.pos_drop = pretrained_vit_encoder.pos_drop\n",
    "        \n",
    "        # until layer n-1, can be changed (memory trade-off)\n",
    "        self.blocks = pretrained_vit_encoder.blocks[:nblocks]        \n",
    "        \n",
    "        # not needed now\n",
    "#         self.norm = pretrained_vit_encoder.norm\n",
    "        \n",
    "        # gradient checkpointing\n",
    "        self.checkpoint_nchunks = checkpoint_nchunks\n",
    "        \n",
    "        self.return_attn_wgts = return_attn_wgts\n",
    "         \n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # collect attn_wgts from all layers\n",
    "        attn_wgts = []\n",
    "        if self.return_attn_wgts:\n",
    "            for i,blk in enumerate(self.blocks):\n",
    "                if i<self.checkpoint_nchunks: x,attn_wgt = checkpoint(blk, x)\n",
    "                else:                         x,attn_wgt = blk(x)\n",
    "                attn_wgts.append(attn_wgt)\n",
    "            return x,attn_wgts\n",
    "        \n",
    "        else:\n",
    "            for i,blk in enumerate(self.blocks):\n",
    "                if i<self.checkpoint_nchunks: x,_ = checkpoint(blk, x)\n",
    "                else:                         x,_ = blk(x)\n",
    "            return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.forward_features(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_attention_maps(attn_wgts, targ_sz=None, mode=None):\n",
    "    \"Generate attention flow maps with shape (targ_sz,targ_sz) from L layer attetion weights of transformer model\"\n",
    "    # Stack for all layers - BS x L x K x gx x gy\n",
    "    att_mat = torch.stack(attn_wgts, dim=1)\n",
    "    # Average the attention weights across all heads.\n",
    "    att_mat = torch.mean(att_mat, dim=2)\n",
    "    # To account for residual connections, we add an identity matrix to the\n",
    "    aug_att_mat = att_mat + torch.eye(att_mat.size(-1))[None,None,...].to(att_mat.device)\n",
    "    # Re-normalize the weights.\n",
    "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "    # Recursively multiply the weight matrices\n",
    "    joint_attentions = aug_att_mat[:,0]\n",
    "    for n in range(1, aug_att_mat.size(1)): joint_attentions = torch.bmm(aug_att_mat[:,n], joint_attentions)\n",
    "\n",
    "    # BS x (num_patches+1) -> BS x gx x gy\n",
    "    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "    joint_attentions = joint_attentions[:,0,1:].view(joint_attentions.size(0),grid_size,grid_size)\n",
    "    joint_attentions /= torch.amax(joint_attentions, dim=(-2,-1), keepdim=True)\n",
    "\n",
    "    # Bilinear interpolation to target size\n",
    "    if mode == 'bilinear':\n",
    "        joint_attentions = F.interpolate(joint_attentions[None,...], \n",
    "                                         (targ_sz,targ_sz), \n",
    "                                         mode=mode, align_corners=True)[0].detach().cpu().numpy()\n",
    "    elif mode == 'nearest':\n",
    "        joint_attentions = F.interpolate(joint_attentions[None,...], \n",
    "                                         (targ_sz,targ_sz), \n",
    "                                         mode=mode)[0].detach().cpu().numpy()\n",
    "    elif mode is None:\n",
    "        joint_attentions = joint_attentions\n",
    "    \n",
    "    return joint_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialTransfomerBlock(Module):\n",
    "    def __init__(self):\n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = nn.Sequential(\n",
    "                            nn.MaxPool2d(2, stride=3),\n",
    "        )\n",
    "        \n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 2*4)\n",
    "        )\n",
    "        \n",
    "        # ranges for each geometrc transform\n",
    "        self.scale_range = SigmoidRange(0,1) # between 0-1 we always crop\n",
    "        self.translate_range = SigmoidRange(-1,1) # stay inside image\n",
    "        \n",
    "    \n",
    "    # Spatial transformer network forward function\n",
    "    def forward(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(xs.size(0), -1)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(x.size(0),2,4)\n",
    "        theta = torch.cat([self.scale_range(theta[:,:,:2]), self.translate_range(theta[:,:,2:])], dim=-1)\n",
    "\n",
    "        # scalex,scaley,translatex,translatey -> affine matrix\n",
    "        # [scalex,      0,   scalex*translatex]\n",
    "        # [0     , scaley,   scaley*translatey]\n",
    "        zeros = torch.zeros(theta.size(0),theta.size(1)).to(theta.device)\n",
    "        theta = torch.stack([theta[:,:,0],\n",
    "                             zeros,\n",
    "                             theta[:,:,0]*theta[:,:,2],\n",
    "                             zeros,\n",
    "                             theta[:,:,1],\n",
    "                             theta[:,:,1]*theta[:,:,3]], dim=-1).view(theta.size(0), theta.size(1), 2, 3)\n",
    "\n",
    "        return theta # BS x 2 (num_crops) x 2 x 3\n",
    "        \n",
    "    def transform(self, x, theta, targ_sz=112):\n",
    "        grid = F.affine_grid(theta, (x.size(0),x.size(1),targ_sz,targ_sz))\n",
    "        out = F.grid_sample(x, grid)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "def generate_batch_attention_maps(attn_wgts, targ_sz=None, mode=None):\n",
    "    \"Generate attention flow maps with shape (targ_sz,targ_sz) from L layer attetion weights of transformer model\"\n",
    "    # Stack for all layers - BS x L x K x gx x gy\n",
    "    att_mat = torch.stack(attn_wgts, dim=1)\n",
    "    \n",
    "    # Average the attention weights across all heads.\n",
    "    att_mat = torch.mean(att_mat, dim=2)\n",
    "   \n",
    "    # To account for residual connections, we add an identity matrix to the\n",
    "    aug_att_mat = att_mat + torch.eye(att_mat.size(-1))[None,None,...].to(att_mat.device)\n",
    "    \n",
    "    # Re-normalize the weights.\n",
    "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "    # Recursively multiply the weight matrices\n",
    "    joint_attentions = aug_att_mat[:,0].clone()\n",
    "    for n in range(1, aug_att_mat.size(1)): joint_attentions = torch.bmm(aug_att_mat[:,n], joint_attentions)\n",
    "\n",
    "    # BS x (num_patches+1) -> BS x gx x gy\n",
    "    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "    joint_attentions = joint_attentions[:,0,1:].view(joint_attentions.size(0),grid_size,grid_size)\n",
    "    joint_attentions /= torch.amax(joint_attentions, dim=(-2,-1), keepdim=True)\n",
    "\n",
    "    # Bilinear interpolation to target size\n",
    "    if mode == 'bilinear':\n",
    "        joint_attentions = F.interpolate(joint_attentions[None,...], \n",
    "                                         (targ_sz,targ_sz), \n",
    "                                         mode=mode, align_corners=True)[0].detach().cpu().numpy()\n",
    "    elif mode == 'nearest':\n",
    "        joint_attentions = F.interpolate(joint_attentions[None,...], \n",
    "                                         (targ_sz,targ_sz), \n",
    "                                         mode=mode)[0].detach().cpu().numpy()\n",
    "    elif mode is None:\n",
    "        joint_attentions = joint_attentions\n",
    "    \n",
    "    return joint_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STViT(Module):\n",
    "    \"Spatial Transformer-ViT Model\"\n",
    "    def __init__(self, pretrained_vit_encoder):\n",
    "        \n",
    "        self.full_image_encoder = FullImageEncoder(pretrained_vit_encoder, nblocks=11, checkpoint_nchunks=12)\n",
    "        self.st_model = SpatialTransfomerBlock()\n",
    "\n",
    "        self.crop_image_encoder = deepcopy(self.full_image_encoder)\n",
    "        self.crop_image_encoder.return_attn_wgts = False\n",
    "        num_patches = 50\n",
    "        # interpolate pos embed from 384 px -> 112 px\n",
    "        pos_embed_data = self.crop_image_encoder.pos_embed.data\n",
    "        new_pos_embed_data = F.interpolate(pos_embed_data[None, ...], size=[num_patches, pos_embed_data.size(-1)])[0]\n",
    "        self.crop_image_encoder.pos_embed.data = new_pos_embed_data\n",
    "        \n",
    "        self.final_block = Block(dim=768,num_heads=12,mlp_ratio=4.,qkv_bias=True,qk_scale=None)\n",
    "        self.norm = partial(nn.LayerNorm, eps=1e-6)(768)\n",
    "        self.classifier = create_cls_module(768, 120, lin_ftrs=[768], use_bn=False, first_bn=False, ps=0.)\n",
    "        \n",
    "              \n",
    "    def forward(self, xb_448):\n",
    "        \n",
    "        xb_384 = F.interpolate(xb_448, size=(384,384))\n",
    "        x_full, attn_wgts = self.full_image_encoder(xb_384)\n",
    "        attention_maps = generate_batch_attention_maps(attn_wgts, None, mode=None)\n",
    "        theta_crops = self.st_model(attention_maps).detach()\n",
    "        \n",
    "#         del attn_wgts\n",
    "#         torch.cuda.empty_cache()\n",
    "        \n",
    "        xb_112_crop1 = self.st_model.transform(xb_448.half(), theta_crops[:,0], targ_sz=112)\n",
    "        xb_112_crop2 = self.st_model.transform(xb_448.half(), theta_crops[:,1], targ_sz=112)\n",
    "        x_crop1 = self.crop_image_encoder(xb_112_crop1)\n",
    "        x_crop2 = self.crop_image_encoder(xb_112_crop2)\n",
    "        \n",
    "        \n",
    "        x = torch.cat([x_full, x_crop1, x_crop2], dim=1)\n",
    "        x,_ = self.final_block(x)\n",
    "        x = self.norm(x)[:,0]\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stvit_model = STViT(pretrained_vit_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = []\n",
    "# if WANDB: cbs += [WandbCallback(log_preds=False,log_model=False)]\n",
    "learn = Learner(dls, stvit_model, opt_func=ranger, cbs=cbs, metrics=[accuracy],\n",
    "#                 splitter=model_splitter,\n",
    "                loss_func=LabelSmoothingCrossEntropyFlat(0.1))\n",
    "learn.to_fp16();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.07585775852203369, lr_steep=0.6309573650360107)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8+yak3AAAACXBIWXMAAAsTAAALEwEAmpwYAAAoiklEQVR4nO3deXhV5bn38e+dmSQkARLCEAKEQUAQQcQBVLBqnapYW2217Wlr5dja9rQ9tqe+fU9PZ9vT1lpPB0tt1bZapzq1Kupr9aAgKvMgIDMkEJIAScg87Pv9IzsxYBISyJ6S3+e6crH3Wmvv9Uvc5s7zPGs9j7k7IiIiAHGRDiAiItFDRUFERNqoKIiISBsVBRERaaOiICIibVQURESkTUKkA/RUdna2jxkzJtIxRERiysqVK8vcPed4x8VcURgzZgwrVqyIdAwRkZhiZru7c5y6j0REpI2KgoiItFFREBGRNioKIiLSRkVBRETaqCiIiEgbFQURkRjw4sZitpdWhfw8KgoiIlEuEHBufWgVj68sDPm5VBRERKLcoZoGGpudYRkpIT9XSIuCme0ys/VmtsbM3ncbspldbWbrWveb2dxQ5hERiUXFFXUADMsMfVEIxzQX8929rJN9LwPPuLub2WnAo8CkMGQSEYkZbUUhDC2FiM595O7tR03SAC0YLSJyjP2V4WsphHpMwYEXzWylmS3s6AAzu8bMNgPPAp/t5JiFwe6lFaWlpSGMKyISfQ5U1BEfZ2SnJ4f8XKEuCnPdfSZwGXCrmZ1/7AHu/qS7TwIWAN/v6E3cfZG7z3L3WTk5x535VUSkTymurGPowGTi4yzk5wppUXD3ouC/JcCTwOwujl0CFJhZdigziYjEmuKKurB0HUEIi4KZpZnZwNbHwCXAhmOOGW9mFnw8E0gGDoYqk4hILCqurAvLIDOEdqA5F3gy+Ds/AXjI3Reb2S0A7n4PcC3wKTNrBGqB691dg80iIu0UV9Qxd3x4OlFCVhTcfQcwvYPt97R7/BPgJ6HKICIS66rqm6iqb2J4rHcfiYjIyQvnjWugoiAiEtVai0JumMYUVBRERKJYcfDGNXUfiYgIxRW1gFoKIiJCS0shKzWRlMT4sJxPRUFEJIoVV9SH7R4FUFEQEYlqxZW1YbvyCFQURESiWnFFfdgGmUFFQUQkajU0BSirqg/bIDOoKIiIRK2SI+FbXKeVioKISJQ6EMbFdVqpKIiIRKn9YZ7iAlQURESiVjjXZm6loiAiEqWKK+pISYwjc0Bi2M6poiAiEqVaF9cJrksTFioKIiJR6kBlXVgvRwUVBRGRqLW/oi6sN66BioKISFQKBJySynpyVRRERORQTQMNzQGGq/tIRETCvQxnKxUFEZEo1Ho3swaaRUSk7W7m4ZkDwnpeFQURkSh0oLKOOIPs9KSwnldFQUQkCh2sbmBQahIJ8eH9NR3Ss5nZLjNbb2ZrzGxFB/tvNLN1wWOWmdn0UOYREYkVFTWNZKaGb3qLVglhOMd8dy/rZN9O4AJ3P2xmlwGLgLPCkElEJKpV1DaGdc6jVuEoCp1y92Xtni4H8iKVRUQkmpTXNpCTnhz284a6s8qBF81spZktPM6xNwHPd7TDzBaa2QozW1FaWtrrIUVEok1fbSnMdfciMxsKvGRmm919ybEHmdl8WorC3I7exN0X0dK1xKxZszyUgUVEokF5TSNZqeG98ghC3FJw96LgvyXAk8DsY48xs9OAe4Gr3f1gKPOIiMSC5oBzpK6JjAi0FEJWFMwszcwGtj4GLgE2HHNMPvAE8El3fzdUWUREYkllbSMAWX2s+ygXeDK4OEQC8JC7LzazWwDc/R7g28AQ4DfB45rcfVYIM4mIRL2KYFHoU2MK7r4DeN99B8Fi0Pr4c8DnQpVBRCQWlbe2FCJwn4LuaBYRiTKRbCmoKIiIRJnymgZALQUREeG9geY+dfWRiIicmPIadR+JiEhQRW0jAxLjSU6ID/u5VRRERKJMeW1jRMYTQEVBRCTqRGreI1BREBGJOioKIiLSpqJGRUFERIIqNKYgIiKtymsb1FIQERGoa2ymrjEQkbUUQEVBRCSqRPJuZlBREBGJKhURXEsBVBRERKJKeQRnSAUVBRGRqFJRE7m1FEBFQUQkqqilICIibd4bU9DVRyIi/V5FTQNmMDAlZKsld0lFQUQkilTUNpKRkkhcnEXk/CoKIiJRpDyCk+GBioKISFSJ5LxHoKIgIhJVyiM4QyqEuCiY2S4zW29ma8xsRQf7J5nZG2ZWb2a3hTKLiEgsqIxw91E4hrfnu3tZJ/sOAV8GFoQhh4hI1OvXYwruXuLubwONkcwhIhIN3L3Pjyk48KKZrTSzhSE+l4hITKuqb6I54H26+2iuuxeZ2VDgJTPb7O5LevomwYKyECA/P7+3M4qIRIVI380MIW4puHtR8N8S4Elg9gm+zyJ3n+Xus3JycnozoohI1CiviexaChDComBmaWY2sPUxcAmwIVTnExGJda0L7ERyTCGU3Ue5wJNm1nqeh9x9sZndAuDu95jZMGAFkAEEzOwrwBR3rwxhLhGRqBTpGVIhhEXB3XcA0zvYfk+7x8VAXqgyiIjEkoooaCnojmYRkSjROqbQb+9TEBGR91TUNpIUH8eAxPiIZVBREBGJEhW1DWQMSCQ4FhsRKgoiIlEi0nczg4qCiEjUiPQMqaCiICISNSpqG8lSURAREVBLQURE2qmsbSRTYwoiItLUHOBIfZNaCiIiApV1TQAaUxAREdh7qAaAnIEpEc2hoiAiEgWWbT8IwJljB0U0h4qCiEgUWLqtjFNyBzJULQURkf6trrGZt3cd4tzxQyIdRUVBRCTSVu0+TH1TgLnjsyMdpXtFIbiKWlzw8UQzu8rMIjtELiLSRyzdXkZ8nHFWQey0FJYAKWY2EngR+CRwf6hCiYj0J69vO8jpo7JITw7lYpjd092iYO5eA3wY+I27fxQ4NXSxRET6h4raRtYXljMnCrqOoAdFwczOAW4Eng1ui9wqECIifcTyHQcJOMwZF/muI+h+UfgKcDvwpLtvNLMC4JWQpRIR6SeWbitjQGI8M/Ije39Cq251YLn7/wL/CxAccC5z9y+HMpiISH+wdFsZZxUMJikhOi4G7e7VRw+ZWYaZpQEbgHfM7OuhjSYi0rftr6hle2k1c8ZFx3gCdL/7aIq7VwILgOeBsbRcgSQiIido6baWqS2iZZAZul8UEoP3JSwAnnH3RsBDlkpEpB9Ytq2MwWlJTBo2MNJR2nS3KPwO2AWkAUvMbDRQebwXmdkuM1tvZmvMbEUH+83M7jazbWa2zsxm9iS8iEgsW723nFmjBxEXZ5GO0qa7A813A3e327TbzOZ38xzz3b2sk32XAROCX2cBvw3+KyLSp1XUNrKzrJqPnJEX6ShH6e5Ac6aZ3WlmK4JfP6el1XCyrgb+5C2WA1lmNrwX3ldEJKptLKoAYNrIzAgnOVp3u4/+CBwBrgt+VQL3deN1DrxoZivNbGEH+0cCe9s9LwxuExHp09ZFaVHo7kQb49z92nbPv2tma7rxurnuXmRmQ4GXzGyzuy/pachgQVkIkJ+f39OXi4hEnfWFFYwaPIBBaUmRjnKU7rYUas1sbusTM5sD1B7vRe5eFPy3BHgSmH3MIUXAqHbP84Lbjn2fRe4+y91n5eTkdDOyiEj0WldUzmkjsyId4326WxRuAX4dvJpoF/Ar4F+7ekFwuu2BrY+BS2i58a29Z4BPBa9COhuocPf9PfkGRERizeHqBvYeqmVaXnR1HUH3rz5aC0w3s4zg80oz+wqwrouX5QJPmlnreR5y98VmdkvwPe4BngMuB7YBNcBnTvD7EBGJGa3jCafFalFoFbyrudXXgLu6OHYHML2D7fe0e+zArT3JICIS69YXlgMwNcoGmeHkluOMnrstRERiyLrCCgqy08hIib4FLE+mKGiaCxGRE7C+qCIqxxPgON1HZnaEjn/5GzAgJIlERPqwkiN17K+oi7r7E1p1WRTcPXpmaRIR6QM2tA0yZ0U2SCeiY1UHEZF+Yl1hBWZw6oiMSEfpkIqCiEgYrSusYHxOOmnJPbr4M2xUFEREwsTdWVdYEbVdR6CiICISNsWVdZRV1UflTWutVBRERMJk9Z5ygKi9HBVUFEREwmbptjLSkuKj9nJUUFEQEQmbpdvKOLtgCInx0furN3qTiYj0IXsP1bDrYA1zxmdHOkqXVBRERMJg2faWpernTlBREBHp917fdpCcgclMGJoe6ShdUlEQEQmxQMBZtq2MueOzCa4xE7VUFEREQmxz8REOVjdE/XgCqCiIiITc0m3B8QQVBREReX1bGeOHpjMsMyXSUY5LRUFEJITqm5p5a+ehmGglgIqCiEivKq9pYNn2MhqbA0DL1Ba1jc0xMZ4Ax1lkR0REum/VnsPc+uAq9lfUMSwjhU+dO5qSynri44yzCgZHOl63qCiIiJwkd+e+pbv40XObGJ6Vwk+uncYza/fx34u3ADAzP4uMlMQIp+weFQURkRN0sKqeZdsP8tTqIl7eXMJFk4fy84+eTmZqItefmc87+yp5+O09XDAxJ9JRu01FQUSkh/65+QA/e+Fd3tlfCcDAlAS+edkkFp5XQFzcezenTRmRwfeunhqpmCck5EXBzOKBFUCRu195zL7RwB+BHOAQ8Al3Lwx1JhGRE1Ve08DXHl3LoNQkvv7BUzh33BCmjcwkIYpnPu2JcLQU/g3YBHS0SvXPgD+5+wNmdiFwB/DJMGQSETkhd770LpW1jTy88GwmDevo11psC2lpM7M84Arg3k4OmQL8M/j4FeDqUOYRETkZm/ZX8pflu/nk2aP7ZEGA0N+ncBfwDSDQyf61wIeDj68BBprZkGMPMrOFZrbCzFaUlpaGJKiISFfcne88s5HMAYl89eKJkY4TMiErCmZ2JVDi7iu7OOw24AIzWw1cABQBzcce5O6L3H2Wu8/KyYmdUXwR6TueXb+fN3ce4rYPnkJWalKk44RMKMcU5gBXmdnlQAqQYWZ/cfdPtB7g7vsIthTMLB241t3LQ5hJRKRbNhRVsHzHQWoamqluaOKp1UVMGZ7Bx87Mj3S0kApZUXD324HbAcxsHnBb+4IQ3J4NHHL3QPDYP4Yqj4hIdzUHnIV/WsG+ijoAkhLiyElP5ofXTCU+LrrXQzhZYb9Pwcy+B6xw92eAecAdZubAEuDWcOcRETnWsu1l7Kuo487rpvOh6SNI7COXm3ZHWIqCu78KvBp8/O122x8HHg9HBhGR7np8ZSEZKQlcPm14vyoIoFlSRUSOUlHbyOINxVx9+khSEuMjHSfsVBRERNp5dt1+6psCfOSMvEhHiQgVBRGRdh5buZeJuemclpcZ6SgRoaIgIhK0raSK1XvK+cgZeZj17auMOqOiICIS9PjKQuLjjAUzRkY6SsSoKIiIAE3NAZ5YVci8iTkMHZgS6TgRo6IgIgK8vq2MkiP1fHRW/xxgbqWiICICPL++mIHJCcyfNDTSUSJKRUFE+r2m5gAvbTrAhZOHkpzQ/+5NaE9FQUT6vbd2HeJQdQOXnjos0lEiTkVBRPq9FzYUk5wQxwWnaGp+FQUR6dcCAeeFjQe4YGIOqUlhnyM06qgoiEi/trawnOLKOi6dqq4jUFEQkX5u8cZiEuKMD0zOjXSUqNCvisKByrpIRxCRKOLuvLChmHPHZ5M5IDHScaJCvykKT60u4pw7XmZ7aVWko4hIlNhy4Ai7DtboqqN2+k1RmDshm4T4OO59bWeko4hImLk7O8uqeXpNEfct3cnyHQc5UtfI8+uLMYOLp6jrqFW/GWrPTk/m2pl5/G1VIf9+yUSy05MjHUlEQmxnWTU/fHYTb+86REVt41H7zCAxLo4zRw8mZ6B+H7TqN0UB4HPnjeWvb+3hT2/s5msXT4x0HBEJEXfnsRWFfOfvG0mMj+PyacOZnpfJaXlZZKcnsXF/JesLK9hcXMn1Z+ZHOm5U6VdFYVxOOhdNzuXPb+zi8xeMY0BS/76dXaQvKj1Sz7ef3sDzG4o5p2AId14/neGZA446ZmhGCvNP6d9zHHWmXxUFgIXnF3Dd7w7w+KpCPnn26EjH6Vd2lFaxak85dY3N1DU209jsXHJqLuNy0iMdTWLc8h0HeWVLCUu3lbFxXyUJccbtl03i5vMKiIvrn4vlnKh+VxTOHDOI6aOy+MNrO7hhdj7x+sCE3K6yau5+eStPrSki4Efv+/mLW/jE2aP5ykUTyEpNikxAiWmvbC7hM/e/TWK8MTN/EF+7aCKXTRvG+KEDIx0tJvW7omBm/Ov5BXzhwVW89E4xl04dHulIUSUQcLaWVLG2sJzmgJMYH0dSQhwDEuMZlJpIVmoSg9OSGJSa2OFyhfvKa9lZVk1ZVT2lR+p5Z18lT6/dR2K88bnzCrhu1igyUhJISYqntqGZX768lT+9sYsnVxfxhXnjWDBjJLkZnS9wEgg4Dc0BAu7EmWEG8WYkxJ/8hXSNzQF2H6xm64Eq3j1QheNMz8vitLxMhujChKj10Ft7yBmYzKu3zSMtud/9Sut1/fIn+MFThzFq8AD+9Mbufl8UAgHnnf2VLNtexps7DrFi9+H3XaXRkez0ZGaNHsSsMYMYmTWAN3ceYsnWUnaUVh913IDEeD51zmg+P2/c+1azykhJ5EfXTONT54zmh89u4o7nN/PjxZuZmT+Iy6YOIyHO2LT/CJuKK9lZWk1tYzNNxzY1aLmK5JyCISw4fSSXThtGRkr3bkJqDjgbiip4fVsZr28tY+XuwzQ0B456Xw+ebmTWAFIS42hoDtDY5CQlxDFp2ECmjMjg1BGZzB4zmMxU3fwUbmVV9byyuYSb5o5VQegl5v7+/8l69QRm8cAKoMjdrzxmXz7wAJAFxAPfdPfnunq/WbNm+YoVK0461x3Pb+IPr+1kzX9dQnof+TDVNTbz9q5DjBmSxqjBqe/b7+4UHq5lS/ERthw4woaiCpbvOMjhmpYiUJCdxpljBnPm2MHMzM8iNSmBxuYA9U0BahuaOVzTwOGaBkqP1LNxXyUrdh9i76FaAFIS4zhr7BDOm5DN1JGZZKcnk5OeTMaAhG4vgL6t5AjPry/muQ3FbNpfCcDgtCQmDx/I+Jx00pITSEpoabnEmxFwCLhTWdvI4o3F7D5YQ1JCHGeOGcSQtGQGpSaSmZpEfVMzh6sbOFzTSHlNA4eqGyivaaS8tpHmYJGZPDyDOeOGcOrIDCYMHUhBThoBhw1FFazdW87GfZU0u5MUH0dSfBxVDU1s2l/JzrJq3CEx3jhvQg5XTBvOxafmdrswycn5w+s7+f4/3uGlr57PhFx1F3XFzFa6+6zjHheGovA1YBaQ0UFRWASsdvffmtkU4Dl3H9PV+/VWUVi2rYwb7n2T339qVkzfuBIIOMt3HuTp1ft4bsN+jtQ1AXDG6EFcffoIpudlsWrPYZbvOMibOw9RXvNeK2Bk1gDOLhjCnPFDOHdcNsMye74u7YHKOgoP13LqiAxSEnvvaq7CwzUkxceRMzC5W0XF3Vmzt5yn1+xj9d5yymsaOFzdQGVdE0kJcQxKTWRQahJZqYkMTksiK7WlC2xi7kDmjM8+4ftWquubeGd/JS+9c4Bn1+2nqLwWMxiekcKowankD07lrIIhfGj68H6/eEtvc3cu++VrJCfG8/StcyIdJ+p1tyiE9E9kM8sDrgB+CHytg0McyAg+zgT2hTJPe2eMGURqUjxL3i2NyaKwpfgIT6wu5Jk1+9hfUUdaUjwfnDqMy6YO590DR3hmzT6+/fTGtuNHDR7AJVNyOX3UIE4ZNpAJuem98tdsbkZKl2MAJypv0PtbOl0xM2bkD2JG/qCjtjcHnDij262VnkpLTmhpXY0ZzO2XTWL13nKWvFvKnoM17DlUwytbSnlsZSH/vXgzn507lhvOylcropds3FfJ5uIjfH/B1EhH6VNC3W9yF/ANoLN23XeAF83sS0AacFFHB5nZQmAhQH5+79xokpwQzzkFQ1iytbRX3u9k1Tc1s66wgqXbyli2/SDbS6oYNzSdaSMzmToyg8T4OHaWVrOzrJqN+yrZcuAI8XHGBRNzuP3yyVw8ObftvouLp+Ry6/zxbC6u5N0DVczMz+rxL9m+IpxXl5m1XP0ys11hcnde21rGoiU7+PHzm/nVP7dx7cyRfPKc0bo65iQ9vrKQpIQ4rjptRKSj9CkhKwpmdiVQ4u4rzWxeJ4d9HLjf3X9uZucAfzazqe4eaH+Quy8CFkFL91FvZTx/Yg4vby5hV1k1Y7LTeuttO7SusJwf/GMTZVX1NAYCNDU7DU2Blmv2mwJtfdtmMHVEJvMnDWV7aRV/Wb6b+qb3fhwjMlMoyEnn47NHceX0EV12e0walsGkYRmd7pfQMzPOn5jD+RNz2FBUwR9e38lf39rLA2/s5pyCIdx4dj4XTc7t1a63/qC+qZmn1hRxyZRcDfD3slC2FOYAV5nZ5UAKkGFmf3H3T7Q75ibgUgB3f8PMUoBsoCSEudqcP7Fl6b0lW0tDVhQCAWfRazv42QtbyBmYzKwxg0mMMxLijcT4OFIS40lJjCMlIZ4JuQM5u2DwUdfrNzUH2F5aTcCdMUPSdBd2DJs6MpNfXH8637piMo+u2MuDy/fwxYdWk5GSwIemj+DDM/OYmZ8Vsq6uvuSVzSWU1zTykTPyIh2lzwn5QDNAsKVwWwcDzc8Dj7j7/WY2GXgZGOldhOqtgWZoadpf8NNXmZibzr3/cmavvGd7Byrr+Ooja1i2/SCXTxvGHdecpr9qpE1zwFm2vYy/rSxk8cZi6hoD5GYkc8HEHOafMpQ5E7I1/tCJm+5/mw37Klj2zQ/oBtRuioqB5o6Y2feAFe7+DPDvwO/N7Ku0DDp/uquCEIIsnD8xmydWFdHQFCApofdmEj9QWcdH7llG2ZEGfnLtNK6bNUp/AcpR4uNaLmM9b0IOVfVNvLChmH9uLuH5DcU8uqKQ+DjjjPxBnD8xm/Mn5jB1RKambKBl5tN/binh1nnjVRBCICwthd7Umy0FgBc3FrPwzyt56OazOHdcNtDSgqhvCpxwP295TQPX/e4Nig7X8tDNZzN9VFav5ZW+r6k5wKo95by6pYQlW0vZUNRyz0ZWaiJnjx3COeOGMHvsYEZkDSAjpfv3gfQV//nUBh55ey+vf3P++26IlM5FbUsh2pwzbggJccaSd8s4d1w2ByrruPXBVWwvreJXN8xkzvjsHr1fdX0Tn77vbXYdrOH+z5ypgiA9lhAfx+yxg5k9djDfuHQSpUfqeX1bKcu2HWTZ9oMs3ljcdmxSfBxD0pOYNjKTW+aNO+rKp77ocHUDj63cy4IZI1QQQqTfF4WBKYmcMXoQS94t5cJJQ7n1oVVU1zeRm5HCJ//wJv/n8sncNHdsh3+NHayq59UtpZRV1RMfZyTEGYs3FrO+qILf3jizreUhcjJyBiZzzYw8rpmRh7uz91Atq/cepvRIPWVVLXeYv7z5AC/+5gDnjhvCrfPHc+64IX2yBfGX5bupawzwufMKIh2lz+r3RQFarkL66QtbuOH3yxk1OJUHP3cWI7IG8PXH1vKDZzexvqiCa2fmUR+8hLSovJaXNx1g5e7D75v1MyHO+O9rT+MSrfkqIWBm5A9JJX/I0fedVNc38de39rBoyQ5uvPdNCrLTWDBjJNfMGNnhlCexqK6xmQfe2M28U3KYqCktQqbfjylAy93Bl9/9GvNPGcqd109vu+LD3fnNq9v52YtbOPbHNGV4BhdNyeWSKbmMzU6jKeDBWUWNgbpiRCKkrrGZZ9bu44lVhSzfcQiAueOz+c5VU2L+ZrlH3t7Df/xtPQ9+7qwed+tKFM191NtCURQASirryE5P7vDqjh2lVRyqbiAlMZ7khDgyUxPVnylRr/BwDU+tLuLe13dSXd/E5+eN5wvzxsXkjXKBgHPJXUtIjI/juS/P7ZNdY6GmgeYeGtrF/D0FOekU5IQxjEgvyBuUyhcvnMDHZufzg3+8w90vb+Uf6/bxi+tOj7kLIP6+bh/bSqr4xfXTVRBCrPcuzBeRqJSdnsxdH5vBA5+dTX1jgOsXvcEL7a5ginYvbizmtsfWMn1UFldM0zxHoaaiINJPXDAxh6e/OIdThmVwy19Wct/SnZGOdFyLNxTzhQdXMWVEJn/67OxevcFUOqafsEg/kp2ezMM3n83Fk3P57t/f4bt/30hDU+D4Lwwzd+fva/fxxYdWMXVkJn++aTaZA3QBRzhoTEGknxmQFM9vP3EGP3j2He5buoul28q448PTOGP04IjmCgSc1XsP8/z6YhZvLKbwcC0z87N44LOzdUVfGOnqI5F+7OVNB/jPpzawv7KOG8/K5+uXTIrIpI0lR+r48l9Xs3zHIZLi45g7IZtLTx3Gh6aP0MzAvURXH4nIcX1gci5nFwzhzpfe5b6lO3n4rb2cXTCED0weygcm5TJq8ICQX+2zYtchvvDgKirrGvn+1ady9YyRmh02gtRSEBEANu2v5Kk1Rby8qYRtJVUADElL4tSRmZw6IoPzJmT36tQt7s59S3fxo+c2kTdoAL/9xBlMHq5FoUJFN6+JyAnbVVbNkq2lrC+sYOO+SraWHKGx2blw0lD+7xWTKchJP6n3P1zdwDf+to6X3jnAxVNy+dlHp2sgOcTUfSQiJ2xMdtpRqxHWNTbz5zd2c/fLW/ngXUv49Llj+MpFE0lL7vmvkDd3HOQrj6yhrKqe/3vFZD47Z6zWiYgiKgoiclwpifHcfH4BC2aM5OcvbuHe13eyufgIf/z0mSTGd//K9geW7eK7f99I/uBUnvj8HKblZYYwtZwI3acgIt2WMzCZH197Gj+59jRe21rGt55cT3e7oA9U1vGj5zZx3oQc/vHl81QQopRaCiLSY9fNGkXh4Vrufnkrowal8qUPTDjua3776naaAs73rj6V9BPodpLw0H8ZETkhX71oAoWHa/j5S+8yImsA156R1+mx+ytqeejNPXxkZh6jh6R1epxEnoqCiJwQM+PHHz6N4oo6vv74WvYcquFLF44noYMxhl+/so2AO1+8cHwEkkpPaExBRE5YUkIcv//ULBbMGMkvX97KDb9/k6Ly2qOOKTxcwyNv7+W6M0f1mVXg+jIVBRE5KWnJCdx53en84vrpbNxXweW/fI1fv7KNDUUVBALOr1/ZhmHcOl+thFig7iMR6RXXzMhjxqhBfOPxdfz0hS389IUtDElLoqK2kRvOymdk1oBIR5RuUFEQkV4zJjuNR285h5LKOl7bWsZrW0vZfahGrYQYEvKiYGbxwAqgyN2vPGbfL4D5waepwFB3zwp1JhEJraEZKVx7Rl6XVyRJdApHS+HfgE3A+2a6cvevtj42sy8BM8KQR0REOhHSgWYzywOuAO7txuEfB/4ayjwiItK1UF99dBfwDaDL9f7MbDQwFvhnJ/sXmtkKM1tRWlra6yFFRKRFyIqCmV0JlLj7ym4c/jHgcXdv7minuy9y91nuPisnJ6dXc4qIyHtC2VKYA1xlZruAh4ELzewvnRz7MdR1JCIScSErCu5+u7vnufsYWn7p/9PdP3HscWY2CRgEvBGqLCIi0j1hv6PZzL5nZle12/Qx4GGPtSXgRET6oLDcvOburwKvBh9/+5h93wlHBhEROb6YW6PZzEqB3UAmUBHcfLzHrf9mA2UncNr279nd/cfbFguZu3re25lPJO+JZu5oW6xkjobPRWcZj5e9v2SO1s/yaHc//pU67h6TX8Ci7j5u9++Kkz1Xd/cfb1ssZO7qeW9nPpG8J5q5k20xkTkaPhfd+Sz058yx+Flu/xXLs6T+vQeP22872XN1d//xtsVC5q6e93bmE8nb0fbuZO7s++ipSGSOhs/Fsdti4bN87DZ9ljsRc91HJ8PMVrj7rEjn6AllDo9YyxxreUGZw+VkM8dyS+FELIp0gBOgzOERa5ljLS8oc7icVOZ+1VIQEZGu9beWgoiIdEFFQURE2qgoiIhIGxWFIDM7z8zuMbN7zWxZpPN0h5nFmdkPzex/zOxfIp2nO8xsnpm9FvxZz4t0nu4ws7Tg1O1XHv/oyDOzycGf7+Nm9vlI5+kOM1tgZr83s0fM7JJI5+kOMyswsz+Y2eORztKZ4Gf3geDP9sbuvKZPFAUz+6OZlZjZhmO2X2pmW8xsm5l9s6v3cPfX3P0W4B/AA6HMG8x20pmBq4E8oBEoDFXWdtl6I7MDVUAKIc7cS3kB/gN4NDQpj9ZLn+VNwc/ydbTMVhxSvZT5KXe/GbgFuD6UeYPZeiPzDne/KbRJ36+H2T9My7IENwNXve/NOnIyd75FyxdwPjAT2NBuWzywHSgAkoC1wBRgGi2/+Nt/DW33ukeBgbGQGfgm8K/B1z4eI5njgq/LBR6MgbwX0zJp46eBK2PhZxx8zVXA88ANsZI5+LqfAzNjLHPI/987iey3A6cHj3moO+8flgnxQs3dl5jZmGM2zwa2ufsOADN7GLja3e8AOuwGMLN8oMLdj4QyL/ROZjMrBBqCTztcoKg39dbPOegwkBySoEG99DOeB6TR8j9YrZk95+5driQY6czB93kGeMbMngUeClXe4Ll64+dswI+B5919VSjzQq9/lsOqJ9lpaY3nAWvoZs9QnygKnRgJ7G33vBA46zivuQm4L2SJjq+nmZ8A/sfMzgOWhDJYF3qU2cw+DHwQyAJ+FdJkHetRXnf/FoCZfRooC2VB6EJPf8bzaOk2SAaeC2WwLvT0s/wl4CIg08zGu/s9oQzXiZ7+nIcAPwRmmNntweIRKZ1lvxv4lZldQTenwejLRaHH3P2/Ip2hJ9y9hpZCFjPc/QlaillMcff7I52hu7zdVPWxwt3vpuUXWMxw94O0jIFELXevBj7Tk9f0iYHmThQBo9o9zwtui2bKHHqxlheUOVxiMXOrXsvel4vC28AEMxtrZkm0DBY+E+FMx6PMoRdreUGZwyUWM7fqvezhHDUP4Wj8X4H9vHdp5k3B7ZcD79IyKv+tSOdUZuVV5uj4isXM4cquCfFERKRNX+4+EhGRHlJREBGRNioKIiLSRkVBRETaqCiIiEgbFQUREWmjoiB9gplVhfl8vbLmhrWsL1FhZmvMbLOZ/awbr1lgZlN64/wix1JREOmAmXU5L5i7n9uLp3vN3U8HZgBXmtnx1kBYQMusrSK9TkVB+iwzG2dmi81spbWs9jYpuP1DZvamma02s/9nZrnB7d8xsz+b2VLgz8HnfzSzV81sh5l9ud17VwX/nRfc/3jwL/0Hg9NAY2aXB7etNLO7zewfXeV191papjgeGXz9zWb2tpmtNbO/mVmqmZ1Ly1oJPw22LsZ19n2KnAgVBenLFgFfcvczgNuA3wS3vw6c7e4zgIeBb7R7zRTgInf/ePD5JFqm+p4N/JeZJXZwnhnAV4KvLQDmmFkK8DvgsuD5c44X1swGARN4bxr0J9z9THefDmyiZTqDZbTMafN1dz/d3bd38X2K9JimzpY+yczSgXOBx4J/uMN7i/rkAY+Y2XBaVqna2e6lzwT/Ym/1rLvXA/VmVkLLinHHLiP6lrsXBs+7BhhDy5KjO9y99b3/CizsJO55ZraWloJwl7sXB7dPNbMf0LL2RDrwQg+/T5EeU1GQvioOKA/21R/rf4A73f2Z4II032m3r/qYY+vbPW6m4/9nunNMV15z9yvNbCyw3Mwedfc1wP3AAndfG1zkZ14Hr+3q+xTpMXUfSZ/k7pXATjP7KLQs92hm04O7M3lvrvl/CVGELUBBu2UTj7sYfbBV8WPgP4KbBgL7g11WN7Y79Ehw3/G+T5EeU1GQviLVzArbfX2Nll+kNwW7ZjbSsmYttLQMHjOzlUBZKMIEu6C+ACwOnucIUNGNl94DnB8sJv8JvAksBTa3O+Zh4OvBgfJxdP59ivSYps4WCREzS3f3quDVSL8Gtrr7LyKdS6QraimIhM7NwYHnjbR0Wf0usnFEjk8tBRERaaOWgoiItFFREBGRNioKIiLSRkVBRETaqCiIiEgbFQUREWnz/wGeryY4B2TDygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.00% [1/100 03:07<5:08:49]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.625048</td>\n",
       "      <td>1.981544</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>03:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='108' class='' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      36.00% [108/300 01:02<01:51 1.6158]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 3e-3\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(int(epochs**2), lr_max=slice(lr/100, lr), pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
