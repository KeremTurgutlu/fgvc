{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from self_supervised.layers import *\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path(\"../data/stanford-dogs-dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(datapath/'train.csv')\n",
    "test_df = pd.read_csv(datapath/'test.csv')\n",
    "sample_df = pd.read_csv(datapath/'sample_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12000, 2), (8580, 2), (6000, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape, sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head()\n",
    "# test_df.head()\n",
    "# sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(filename): return PILImage.create(datapath/'images/Images'/filename)\n",
    "def read_image_size(filename): return PILImage.create(datapath/'images/Images'/filename).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FAST:\n",
    "    filenames = sample_df['filename'].values\n",
    "    labels = sample_df['label'].values\n",
    "    fn2label = dict(zip(filenames, labels))\n",
    "else:\n",
    "    filenames = train_df['filenames'].values\n",
    "    labels = train_df['labels'].values\n",
    "    fn2label = dict(zip(filenames, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label(filename): return fn2label[filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_filenames = sample_df.query(\"split == 'valid'\")['filename'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size,bs = 384,32\n",
    "\n",
    "tfms = [[read_image, ToTensor, RandomResizedCrop(size, min_scale=.75)], \n",
    "        [read_label, Categorize()]]\n",
    "\n",
    "valid_splitter = lambda o: True if o in valid_filenames else False \n",
    "dsets = Datasets(filenames, tfms=tfms, splits=FuncSplitter(valid_splitter)(filenames))\n",
    "\n",
    "batch_augs = aug_transforms()\n",
    "# batch_augs = []\n",
    "\n",
    "stats = imagenet_stats\n",
    "\n",
    "batch_tfms = [IntToFloatTensor] + batch_augs + [Normalize.from_stats(*stats)]\n",
    "dls = dsets.dataloaders(bs=bs, after_batch=batch_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4800, 1200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dls.train_ds), len(dls.valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifications on ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_vit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timm vit _encoder\n",
    "arch = \"vit_base_patch16_384\"\n",
    "_encoder = create_encoder(arch, pretrained=True, n_in=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom vit encoder with timm weights\n",
    "encoder = VisionTransformer(img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12)\n",
    "encoder.head = Identity()\n",
    "encoder.load_state_dict(_encoder.state_dict());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad checkpointing\n",
    "encoder = CheckpointVisionTransformer(encoder, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Change Stride Size\n",
    "\n",
    "patch_size,stride_size = 16,16\n",
    "\n",
    "# new_patch_embed = PatchEmbed(size, patch_size, stride_size)\n",
    "# new_patch_embed.proj.weight.data = encoder.vit_model.patch_embed.proj.weight.data\n",
    "# new_patch_embed.proj.bias.data = encoder.vit_model.patch_embed.proj.bias.data\n",
    "# encoder.vit_model.patch_embed = new_patch_embed\n",
    "\n",
    "# 2) Interpolate Position Embeddings to new Number of Patches\n",
    "\n",
    "# num_patches = ((size - patch_size + stride_size) // stride_size)**2 + 1\n",
    "\n",
    "# pos_embed_data = encoder.vit_model.pos_embed.data\n",
    "# new_pos_embed_data = F.interpolate(pos_embed_data[None, ...], \n",
    "#                                    size=[num_patches, pos_embed_data.size(-1)], \n",
    "#                                    mode='nearest')[0]\n",
    "# encoder.vit_model.pos_embed.data = new_pos_embed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out, attn_wgts = encoder(torch.randn(2,3,size,size))\n",
    "    nf = out.size(1)\n",
    "classifier = create_cls_module(nf, dls.c, lin_ftrs=[768], use_bn=False, first_bn=False, ps=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGVCModel(Module):\n",
    "    def __init__(self, encoder, classifier, return_attn_wgts=False):\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "        self.return_attn_wgts = return_attn_wgts\n",
    "        \n",
    "    def forward(self, x):\n",
    "        cls_token,attn_wgts = self.encoder(x)\n",
    "        if self.return_attn_wgts: return self.classifier(cls_token), attn_wgts, cls_token\n",
    "        else:                     return self.classifier(cls_token)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FGVCModel(encoder, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 577, 577])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_wgts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Linear(in_features=768, out_features=120, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_splitter(m): return L(m[0], m[1]).map(params)\n",
    "def model_splitter(m): return L(m.encoder, m.classifier).map(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = []\n",
    "# if WANDB: cbs += [WandbCallback(log_preds=False,log_model=False)]\n",
    "learn = Learner(dls, model, opt_func=ranger, cbs=cbs, metrics=[accuracy], splitter=model_splitter,\n",
    "                loss_func=LabelSmoothingCrossEntropyFlat(0.1))\n",
    "learn.to_fp16();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 2\n",
    "\n",
    "# lr = 3e-3\n",
    "# learn.freeze()\n",
    "# learn.fit_one_cycle(epochs, lr_max=(lr), pct_start=0.5)\n",
    "\n",
    "# lr /= 3 \n",
    "# learn.unfreeze()\n",
    "# learn.fit_one_cycle(int(epochs**2), lr_max=slice(lr/100, lr), pct_start=0.5)\n",
    "\n",
    "# learn.save(f\"{arch}_stride_{stride_size}_imsize_{size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f\"{arch}_stride_{stride_size}_imsize_384\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vit_encoder = learn.model.encoder.vit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del learn, dls, model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Crop Dataset : 1 x (384 px whole image) + 2 x (448 px -> 112 px crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.attention import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "size,bs = 448,16\n",
    "\n",
    "tfms = [[read_image, ToTensor, RandomResizedCrop(size, min_scale=.75)], \n",
    "        [read_label, Categorize()]]\n",
    "\n",
    "valid_splitter = lambda o: True if o in valid_filenames else False \n",
    "dsets = Datasets(filenames, tfms=tfms, splits=FuncSplitter(valid_splitter)(filenames))\n",
    "\n",
    "batch_augs = aug_transforms()\n",
    "# batch_augs = []\n",
    "\n",
    "stats = imagenet_stats\n",
    "\n",
    "batch_tfms = [IntToFloatTensor] + batch_augs + [Normalize.from_stats(*stats)]\n",
    "dls = dsets.dataloaders(bs=bs, after_batch=batch_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint\n",
    "class FullImageEncoder(Module):\n",
    "    \"Encoder which takes whole image input then outputs attention weights + layer features\"\n",
    "    def __init__(self, pretrained_vit_encoder, nblocks=11, checkpoint_nchunks=2, return_attn_wgts=True):\n",
    "                \n",
    "        # initialize params with warm up model\n",
    "        self.patch_embed = pretrained_vit_encoder.patch_embed\n",
    "        self.cls_token = pretrained_vit_encoder.cls_token\n",
    "        self.pos_embed = pretrained_vit_encoder.pos_embed\n",
    "        self.pos_drop = pretrained_vit_encoder.pos_drop\n",
    "        \n",
    "        # until layer n-1, can be changed (memory trade-off)\n",
    "        self.blocks = pretrained_vit_encoder.blocks[:nblocks]        \n",
    "        \n",
    "        # not needed now\n",
    "#         self.norm = pretrained_vit_encoder.norm\n",
    "        \n",
    "        # gradient checkpointing\n",
    "        self.checkpoint_nchunks = checkpoint_nchunks\n",
    "        \n",
    "        self.return_attn_wgts = return_attn_wgts\n",
    "         \n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # collect attn_wgts from all layers\n",
    "        attn_wgts = []\n",
    "        if self.return_attn_wgts:\n",
    "            for i,blk in enumerate(self.blocks):\n",
    "                if i<self.checkpoint_nchunks: x,attn_wgt = checkpoint(blk, x)\n",
    "                else:                         x,attn_wgt = blk(x)\n",
    "                attn_wgts.append(attn_wgt)\n",
    "            return x,attn_wgts\n",
    "        \n",
    "        else:\n",
    "            for i,blk in enumerate(self.blocks):\n",
    "                if i<self.checkpoint_nchunks: x,_ = checkpoint(blk, x)\n",
    "                else:                         x,_ = blk(x)\n",
    "            return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.forward_features(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_attention_maps(attn_wgts, targ_sz=None, mode=None):\n",
    "    \"Generate attention flow maps with shape (targ_sz,targ_sz) from L layer attetion weights of transformer model\"\n",
    "    # Stack for all layers - BS x L x K x gx x gy\n",
    "    att_mat = torch.stack(attn_wgts, dim=1)\n",
    "    # Average the attention weights across all heads.\n",
    "    att_mat = torch.mean(att_mat, dim=2)\n",
    "    # To account for residual connections, we add an identity matrix to the\n",
    "    aug_att_mat = att_mat + torch.eye(att_mat.size(-1))[None,None,...].to(att_mat.device)\n",
    "    # Re-normalize the weights.\n",
    "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "    # Recursively multiply the weight matrices\n",
    "    joint_attentions = aug_att_mat[:,0]\n",
    "    for n in range(1, aug_att_mat.size(1)): joint_attentions = torch.bmm(aug_att_mat[:,n], joint_attentions)\n",
    "\n",
    "    # BS x (num_patches+1) -> BS x gx x gy\n",
    "    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "    joint_attentions = joint_attentions[:,0,1:].view(joint_attentions.size(0),grid_size,grid_size)\n",
    "    joint_attentions /= torch.amax(joint_attentions, dim=(-2,-1), keepdim=True)\n",
    "\n",
    "    # Bilinear interpolation to target size\n",
    "    if mode == 'bilinear':\n",
    "        joint_attentions = F.interpolate(joint_attentions[None,...], \n",
    "                                         (targ_sz,targ_sz), \n",
    "                                         mode=mode, align_corners=True)[0].detach().cpu().numpy()\n",
    "    elif mode == 'nearest':\n",
    "        joint_attentions = F.interpolate(joint_attentions[None,...], \n",
    "                                         (targ_sz,targ_sz), \n",
    "                                         mode=mode)[0].detach().cpu().numpy()\n",
    "    elif mode is None:\n",
    "        joint_attentions = joint_attentions\n",
    "    \n",
    "    return joint_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialTransfomerBlock(Module):\n",
    "    def __init__(self):\n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = nn.Sequential(\n",
    "                            nn.MaxPool2d(2, stride=3),\n",
    "        )\n",
    "        \n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 2*4)\n",
    "        )\n",
    "        \n",
    "        # ranges for each geometrc transform\n",
    "        self.scale_range = SigmoidRange(0,1) # between 0-1 we always crop\n",
    "        self.translate_range = SigmoidRange(-1,1) # stay inside image\n",
    "        \n",
    "    \n",
    "    # Spatial transformer network forward function\n",
    "    def forward(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(xs.size(0), -1)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(x.size(0),2,4)\n",
    "        theta = torch.cat([self.scale_range(theta[:,:,:2]), self.translate_range(theta[:,:,2:])], dim=-1)\n",
    "\n",
    "        # scalex,scaley,translatex,translatey -> affine matrix\n",
    "        # [scalex,      0,   scalex*translatex]\n",
    "        # [0     , scaley,   scaley*translatey]\n",
    "        zeros = torch.zeros(theta.size(0),theta.size(1)).to(theta.device)\n",
    "        theta = torch.stack([theta[:,:,0],\n",
    "                             zeros,\n",
    "                             theta[:,:,0]*theta[:,:,2],\n",
    "                             zeros,\n",
    "                             theta[:,:,1],\n",
    "                             theta[:,:,1]*theta[:,:,3]], dim=-1).view(theta.size(0), theta.size(1), 2, 3)\n",
    "\n",
    "        return theta # BS x 2 (num_crops) x 2 x 3\n",
    "        \n",
    "    def transform(self, x, theta, targ_sz=112):\n",
    "        grid = F.affine_grid(theta, (x.size(0),x.size(1),targ_sz,targ_sz))\n",
    "        out = F.grid_sample(x, grid)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "def generate_batch_attention_maps(attn_wgts, targ_sz=None, mode=None):\n",
    "    \"Generate attention flow maps with shape (targ_sz,targ_sz) from L layer attetion weights of transformer model\"\n",
    "    # Stack for all layers - BS x L x K x gx x gy\n",
    "    att_mat = torch.stack(attn_wgts, dim=1)\n",
    "    \n",
    "    # Average the attention weights across all heads.\n",
    "    att_mat = torch.mean(att_mat, dim=2)\n",
    "   \n",
    "    # To account for residual connections, we add an identity matrix to the\n",
    "    aug_att_mat = att_mat + torch.eye(att_mat.size(-1))[None,None,...].to(att_mat.device)\n",
    "    \n",
    "    # Re-normalize the weights.\n",
    "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "    # Recursively multiply the weight matrices\n",
    "    joint_attentions = aug_att_mat[:,0].clone()\n",
    "    for n in range(1, aug_att_mat.size(1)): joint_attentions = torch.bmm(aug_att_mat[:,n], joint_attentions)\n",
    "\n",
    "    # BS x (num_patches+1) -> BS x gx x gy\n",
    "    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "    joint_attentions = joint_attentions[:,0,1:].view(joint_attentions.size(0),grid_size,grid_size)\n",
    "    joint_attentions /= torch.amax(joint_attentions, dim=(-2,-1), keepdim=True)\n",
    "\n",
    "    # Bilinear interpolation to target size\n",
    "    if mode == 'bilinear':\n",
    "        joint_attentions = F.interpolate(joint_attentions[None,...], \n",
    "                                         (targ_sz,targ_sz), \n",
    "                                         mode=mode, align_corners=True)[0].detach().cpu().numpy()\n",
    "    elif mode == 'nearest':\n",
    "        joint_attentions = F.interpolate(joint_attentions[None,...], \n",
    "                                         (targ_sz,targ_sz), \n",
    "                                         mode=mode)[0].detach().cpu().numpy()\n",
    "    elif mode is None:\n",
    "        joint_attentions = joint_attentions\n",
    "    \n",
    "    return joint_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STViT(Module):\n",
    "    \"Spatial Transformer-ViT Model\"\n",
    "    def __init__(self, pretrained_vit_encoder):\n",
    "        \n",
    "        self.full_image_encoder = FullImageEncoder(pretrained_vit_encoder, nblocks=11, checkpoint_nchunks=12)\n",
    "        self.st_model = SpatialTransfomerBlock()\n",
    "\n",
    "        self.crop_image_encoder = deepcopy(self.full_image_encoder)\n",
    "        self.crop_image_encoder.return_attn_wgts = False\n",
    "        num_patches = 50\n",
    "        # interpolate pos embed from 384 px -> 112 px\n",
    "        pos_embed_data = self.crop_image_encoder.pos_embed.data\n",
    "        new_pos_embed_data = F.interpolate(pos_embed_data[None, ...], size=[num_patches, pos_embed_data.size(-1)])[0]\n",
    "        self.crop_image_encoder.pos_embed.data = new_pos_embed_data\n",
    "        \n",
    "        self.final_block = Block(dim=768,num_heads=12,mlp_ratio=4.,qkv_bias=True,qk_scale=None)\n",
    "        self.norm = partial(nn.LayerNorm, eps=1e-6)(768)\n",
    "        self.classifier = create_cls_module(768, 120, lin_ftrs=[768], use_bn=False, first_bn=False, ps=0.)\n",
    "        \n",
    "              \n",
    "    def forward(self, xb_448):\n",
    "        \n",
    "        xb_384 = F.interpolate(xb_448, size=(384,384))\n",
    "        x_full, attn_wgts = self.full_image_encoder(xb_384)\n",
    "\n",
    "        attention_maps = generate_batch_attention_maps(attn_wgts, None, mode=None).detach()\n",
    "        \n",
    "        theta_crops = self.st_model(attention_maps)\n",
    "        \n",
    "#         del attn_wgts\n",
    "#         torch.cuda.empty_cache()\n",
    "        \n",
    "        xb_112_crop1 = self.st_model.transform(xb_448.half(), theta_crops[:,0], targ_sz=112)\n",
    "        xb_112_crop2 = self.st_model.transform(xb_448.half(), theta_crops[:,1], targ_sz=112)\n",
    "        x_crop1 = self.crop_image_encoder(xb_112_crop1)\n",
    "        x_crop2 = self.crop_image_encoder(xb_112_crop2)\n",
    "        \n",
    "        \n",
    "        x = torch.cat([x_full, x_crop1, x_crop2], dim=1)\n",
    "        x,_ = self.final_block(x)\n",
    "        x = self.norm(x)[:,0]\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_splitter(m): return L(m.full_image_encoder, \n",
    "                                m.st_model, \n",
    "                                m.crop_image_encoder,\n",
    "                                m.final_block, \n",
    "                                m.norm,\n",
    "                                m.classifier).map(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stvit_model = STViT(pretrained_vit_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = []\n",
    "# if WANDB: cbs += [WandbCallback(log_preds=False,log_model=False)]\n",
    "learn = Learner(dls, stvit_model, opt_func=ranger, cbs=cbs, metrics=[accuracy],\n",
    "                splitter=model_splitter,\n",
    "                loss_func=LabelSmoothingCrossEntropyFlat(0.1))\n",
    "learn.to_fp16();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/4 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='162' class='' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      54.00% [162/300 00:52<00:44 4.0962]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 4\n",
    "\n",
    "lr = 3e-3\n",
    "learn.freeze_to(1)\n",
    "learn.fit_one_cycle(epochs, lr_max=(lr), pct_start=0.5)\n",
    "\n",
    "lr /= 3 \n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(int(epochs**2), lr_max=[lr/100, lr, lr, lr, lr, lr], pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
