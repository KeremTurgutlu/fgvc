{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from self_supervised.layers import *\n",
    "from self_supervised.multimodal.clip import *\n",
    "import clip\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path(\"plant-pathology-2021-fgvc8/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#5) [Path('plant-pathology-2021-fgvc8/train.csv'),Path('plant-pathology-2021-fgvc8/test_images'),Path('plant-pathology-2021-fgvc8/train_images_512'),Path('plant-pathology-2021-fgvc8/train_images'),Path('plant-pathology-2021-fgvc8/sample_submission.csv')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(datapath/'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dist = train_df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scab                               4826\n",
       "healthy                            4624\n",
       "frog_eye_leaf_spot                 3181\n",
       "cider_apple_rust                   1860\n",
       "complex                            1602\n",
       "powdery_mildew                     1184\n",
       "scab frog_eye_leaf_spot             686\n",
       "scab frog_eye_leaf_spot complex     200\n",
       "frog_eye_leaf_spot complex          165\n",
       "rust frog_eye_leaf_spot             120\n",
       "rust complex                         97\n",
       "powdery_mildew complex               87\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['num_disease'] = train_df['labels'].apply(lambda o: len(o.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scab                  4826\n",
       "healthy               4624\n",
       "frog_eye_leaf_spot    3181\n",
       "cider_apple_rust      1860\n",
       "complex               1602\n",
       "powdery_mildew        1184\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.query(\"num_disease == 1\")['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scab frog_eye_leaf_spot            686\n",
       "scab frog_eye_leaf_spot complex    200\n",
       "frog_eye_leaf_spot complex         165\n",
       "rust frog_eye_leaf_spot            120\n",
       "rust complex                        97\n",
       "powdery_mildew complex              87\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.query(\"num_disease > 1\")['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_img(im): return PILImage.create(datapath/f'train_images/{im}')\n",
    "\n",
    "# img = read_img(train_df['image'][0])\n",
    "\n",
    "# img.shape\n",
    "\n",
    "# def resize_img(img, sz=512):\n",
    "#     targ_sz = resize_to(img, use_min=True, targ_sz=sz)\n",
    "#     return img.resize(targ_sz, resample=Image.BILINEAR).convert(\"RGB\")\n",
    "\n",
    "# resizepath = datapath/'train_images_512'\n",
    "\n",
    "# resizepath.mkdir()\n",
    "\n",
    "# fns = train_df['image'].values; fns[:5]\n",
    "\n",
    "# def read_resize_save(fn):\n",
    "#     img = read_img(fn)\n",
    "#     img = resize_img(img)\n",
    "#     img.save(resizepath/f'{fn}')\n",
    "\n",
    "# # parallel(read_resize_save, fns, progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = train_df['image'].values\n",
    "fn2label = dict(zip(train_df['image'], train_df['labels'].apply(lambda o: o.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(im): return PILImage.create(datapath/f'train_images_512/{im}')\n",
    "def read_label(fn): return fn2label[fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(files, bs=32, size=448, stats=imagenet_stats):\n",
    "\n",
    "    tfms = [[read_img, ToTensor, RandomResizedCrop(size, min_scale=.7)], \n",
    "            [read_label, MultiCategorize(), OneHotEncode]]\n",
    "\n",
    "    dsets = Datasets(files, tfms=tfms, splits=RandomSplitter(valid_pct=0.1, seed=0.2)(files))\n",
    "\n",
    "    batch_augs = aug_transforms()\n",
    "\n",
    "    batch_tfms = [IntToFloatTensor] + batch_augs + [Normalize.from_stats(*stats)]\n",
    "    dls = dsets.dataloaders(bs=bs, after_batch=batch_tfms)\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from fastai.callback.wandb import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_efficientnet_b4_ns - lr 2e-2, size 448, bs 32\n",
    "# resnet101d - lr 2e-2, size 448, bs 32\n",
    "# vit_base_patch16_384 - lr 1e-3 size 384, bs 32 (checkpoint 2)\n",
    "# vit_base_resnet50_384 - lr 1e-3 size 384 bs 24 (checkpoint 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export \n",
    "# from torch.utils.checkpoint import checkpoint_sequential\n",
    "    \n",
    "# class CheckpointVisionTransformer(Module):\n",
    "#     def __init__(self, vit_model, checkpoint_nchunks=2):\n",
    "#         self.checkpoint_nchunks = checkpoint_nchunks\n",
    "#         self.vit_model = vit_model\n",
    "    \n",
    "#     def forward_features(self, x):\n",
    "#         B = x.shape[0]\n",
    "#         x = self.vit_model.patch_embed(x)\n",
    "\n",
    "#         cls_tokens = self.vit_model.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "#         x = x + self.vit_model.pos_embed\n",
    "#         x = self.vit_model.pos_drop(x)\n",
    "#         x = checkpoint_sequential(self.vit_model.blocks, self.checkpoint_nchunks, x)\n",
    "#         x = self.vit_model.norm(x)[:, 0]\n",
    "#         x = self.vit_model.pre_logits(x)\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.forward_features(x)\n",
    "#         x = self.vit_model.head(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Tuple, Union\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = None\n",
    "        self.stride = stride\n",
    "\n",
    "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
    "            self.downsample = nn.Sequential(OrderedDict([\n",
    "                (\"-1\", nn.AvgPool2d(stride)),\n",
    "                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n",
    "                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.avgpool(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionPool2d(nn.Module):\n",
    "    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n",
    "        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n",
    "        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n",
    "        x, _ = F.multi_head_attention_forward(\n",
    "            query=x, key=x, value=x,\n",
    "            embed_dim_to_check=x.shape[-1],\n",
    "            num_heads=self.num_heads,\n",
    "            q_proj_weight=self.q_proj.weight,\n",
    "            k_proj_weight=self.k_proj.weight,\n",
    "            v_proj_weight=self.v_proj.weight,\n",
    "            in_proj_weight=None,\n",
    "            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n",
    "            bias_k=None,\n",
    "            bias_v=None,\n",
    "            add_zero_attn=False,\n",
    "            dropout_p=0,\n",
    "            out_proj_weight=self.c_proj.weight,\n",
    "            out_proj_bias=self.c_proj.bias,\n",
    "            use_separate_proj_weight=True,\n",
    "            training=self.training,\n",
    "            need_weights=False\n",
    "        )\n",
    "\n",
    "        return x[0]\n",
    "\n",
    "\n",
    "class ModifiedResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet class that is similar to torchvision's but contains the following changes:\n",
    "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
    "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
    "    - The final pooling layer is a QKV attention instead of an average pool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        # the 3-layer stem\n",
    "        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(width)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # residual layers\n",
    "        self._inplanes = width  # this is a *mutable* variable used during construction\n",
    "        self.layer1 = self._make_layer(width, layers[0])\n",
    "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n",
    "\n",
    "        embed_dim = width * 32  # the ResNet feature dimension\n",
    "        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "\n",
    "        self._inplanes = planes * Bottleneck.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self._inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        def stem(x):\n",
    "            for conv, bn in [(self.conv1, self.bn1), (self.conv2, self.bn2), (self.conv3, self.bn3)]:\n",
    "                x = self.relu(bn(conv(x)))\n",
    "            x = self.avgpool(x)\n",
    "            return x\n",
    "\n",
    "        x = x.type(self.conv1.weight.dtype)\n",
    "        x = stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.attnpool(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "            (\"gelu\", QuickGELU()),\n",
    "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "    def attention(self, x: torch.Tensor):\n",
    "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
    "        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None, checkpoint=False, checkpoint_nchunks=2):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
    "        self.checkpoint = checkpoint\n",
    "        self.checkpoint_nchunks = checkpoint_nchunks\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.checkpoint: return torch.utils.checkpoint.checkpoint_sequential(self.resblocks, self.checkpoint_nchunks, x)\n",
    "        else:               return self.resblocks(x)\n",
    "\n",
    "\n",
    "class VisualTransformer(nn.Module):\n",
    "    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int, patch_stride=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "        if patch_stride is None: patch_stride = patch_size\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_stride, bias=False)\n",
    "\n",
    "        scale = width ** -0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        \n",
    "        \n",
    "        num_pathces = (input_resolution - patch_size + patch_stride) // patch_stride\n",
    "        \n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn(num_pathces ** 2 + 1, width))\n",
    "        self.ln_pre = LayerNorm(width)\n",
    "\n",
    "        self.transformer = Transformer(width, layers, heads, **kwargs)\n",
    "\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "        x = self.ln_pre(x)\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "        x = self.ln_post(x[:, 0, :])\n",
    "\n",
    "        if self.proj is not None:\n",
    "            x = x @ self.proj\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 # vision\n",
    "                 image_resolution: int,\n",
    "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "                 vision_width: int,\n",
    "                 vision_patch_size: int,\n",
    "                 # text\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        if isinstance(vision_layers, (tuple, list)):\n",
    "            vision_heads = vision_width * 32 // 64\n",
    "            self.visual = ModifiedResNet(\n",
    "                layers=vision_layers,\n",
    "                output_dim=embed_dim,\n",
    "                heads=vision_heads,\n",
    "                input_resolution=image_resolution,\n",
    "                width=vision_width\n",
    "            )\n",
    "        else:\n",
    "            vision_heads = vision_width // 64\n",
    "            self.visual = VisualTransformer(\n",
    "                input_resolution=image_resolution,\n",
    "                patch_size=vision_patch_size,\n",
    "                width=vision_width,\n",
    "                layers=vision_layers,\n",
    "                heads=vision_heads,\n",
    "                output_dim=embed_dim,\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask=self.build_attention_mask(),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
    "        self.ln_final = LayerNorm(transformer_width)\n",
    "\n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1/0.07))) # Same initialization as paper\n",
    "\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "        \n",
    "        # visual model\n",
    "        if isinstance(self.visual, ModifiedResNet):\n",
    "            if self.visual.attnpool is not None:\n",
    "                std = self.visual.attnpool.c_proj.in_features ** -0.5\n",
    "                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n",
    "\n",
    "            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n",
    "                for name, param in resnet_block.named_parameters():\n",
    "                    if name.endswith(\"bn3.weight\"):\n",
    "                        nn.init.zeros_(param)\n",
    "        \n",
    "        # text model\n",
    "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
    "        attn_std = self.transformer.width ** -0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
    "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
    "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
    "\n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
    "\n",
    "    def build_attention_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(self.context_length, self.context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        return mask\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.conv1.weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(text)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return image_features, text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vitb_model(size, patch_size=32, patch_stride=32, clip_pretrained=True, checkpoint=False, checkpoint_nchunks=2):\n",
    "    \"ViT-B model with 12 layers, 12 heads, width 768, embed size 512 and custom patch size and stride size\"\n",
    "    if clip_pretrained:\n",
    "        if patch_size != 32 and patch_stride != 32: raise Exception(f\"Patch size and stride needs to be 32 for pretrained model\")\n",
    "        print(\"Loading pretrained model..\")\n",
    "        vitb32_config_dict = vitb32_config(224, context_length=77, vocab_size=49408)\n",
    "        clip_model = CLIP(**vitb32_config_dict, checkpoint_nchunks=checkpoint_nchunks)\n",
    "        clip_pretrained_model, _ = clip.load(\"ViT-B/32\", jit=False)\n",
    "        clip_model.load_state_dict(clip_pretrained_model.state_dict())\n",
    "\n",
    "        clip_vitb = clip_model.visual\n",
    "        num_patches = (size//patch_size)**2 +1\n",
    "        # interpolate positional embedding to match any input size\n",
    "        embed_dim = clip_vitb.positional_embedding.size(1)\n",
    "        clip_vitb.positional_embedding.data = F.interpolate(clip_vitb.positional_embedding.data[None, None, ...], \n",
    "                                                            size=[num_patches, embed_dim], \n",
    "                                                            mode='bilinear',align_corners=False)[0,0]\n",
    "        del clip_model, clip_pretrained_model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        clip_vitb = VisualTransformer(size,patch_size=patch_size,patch_stride=patch_stride,\n",
    "                                      width=768,layers=12,heads=12,output_dim=512,\n",
    "                                      checkpoint=checkpoint, checkpoint_nchunks=checkpoint_nchunks)\n",
    "    \n",
    "    return clip_vitb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualTransformer(nn.Module):\n",
    "    def __init__(self, input_resolution: int, patch_sizes: list, width: int, layers: int, heads: int, output_dim: int, patch_strides=None, **kwargs):\n",
    "        \"A Vision Transformer with sliding patch and multi scale patch support\"\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        if isinstance(patch_sizes, int):   patch_sizes   = listify(patch_sizes)\n",
    "        if isinstance(patch_strides, int): patch_strides = listify(patch_strides) \n",
    "        if patch_strides is None: patch_strides = patch_sizes\n",
    "        \n",
    "        self.convs = nn.ModuleList([])\n",
    "        num_patches = 0\n",
    "        for patch_sz, stride_sz in zip(patch_sizes, patch_strides):\n",
    "            self.convs += [nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_sz, stride=stride_sz, bias=False)]\n",
    "            num_patches += ((input_resolution - patch_sz + stride_sz) // stride_sz)**2\n",
    "            \n",
    "        scale = width ** -0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        \n",
    "                \n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn(num_patches + 1, width))\n",
    "        self.ln_pre = LayerNorm(width)\n",
    "\n",
    "        self.transformer = Transformer(width, layers, heads, **kwargs)\n",
    "\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
    "\n",
    "    def forward(self, inp: torch.Tensor):\n",
    "        patch_embeds = []\n",
    "        for conv in self.convs:\n",
    "            out = conv(inp)  # shape = [*, width, grid, grid]\n",
    "            out = out.reshape(out.shape[0], out.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "            patch_embeds.append(out)\n",
    "        \n",
    "        x = torch.cat(patch_embeds, dim=-1)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "        x = self.ln_pre(x)\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "        x = self.ln_post(x[:, 0, :])\n",
    "\n",
    "        if self.proj is not None:\n",
    "            x = x @ self.proj\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to much memory: 16 / 8 : bs 16 size 384 lr 1e-4 gradcheck 0\n",
    "# 16 / 16 : bs 16 size 384 lr 1e-4 gradcheck 0 (24 x 24)\n",
    "# 32 / 16 : bs 16 size 384 lr 1e-4 gradcheck 2 (23 x 23)\n",
    "# 32 / 32 : bs 32 size 384 lr 1e-4 gradcheck 0 (12 x 12)\n",
    "# 64 / 32 : bs 32 size 384 lr 1e-4 gradcheck 0 (11 x 11)\n",
    "# 64 / 64 : bs 32 size 384 lr 1e-4 gradcheck 0 (6 x 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, size, lr, epochs = 16, 384, 2e-2, 24\n",
    "patch_sizes, patch_strides = [32, 16], None\n",
    "clip_pretrained=False\n",
    "checkpoint=True\n",
    "checkpoint_nchunks=2\n",
    "dls = get_dls(files,bs,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WANDB:\n",
    "    xtra_config = {\"Arch\":arch, \"Size\":size, \"lr\": lr}\n",
    "    wandb.init(project=\"plant-pathology-2021\", config=xtra_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def name(self:F1ScoreMulti): return \"f1_multi_macro\"\n",
    "f1macro = F1ScoreMulti(average='macro')\n",
    "@patch\n",
    "def name(self:F1ScoreMulti): return \"f1_multi_micro\"\n",
    "f1micro = F1ScoreMulti(average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "encoder = VisualTransformer(size,patch_sizes=patch_sizes, patch_strides=patch_strides,\n",
    "                                      width=768,layers=12,heads=12,output_dim=512,\n",
    "                                      checkpoint=checkpoint, checkpoint_nchunks=checkpoint_nchunks)\n",
    "with torch.no_grad(): nf = encoder(torch.randn(2,3,size,size)).size(-1)\n",
    "classifier = create_cls_module(nf, dls.c)\n",
    "model = nn.Sequential(encoder, classifier)\n",
    "cbs = [SaveModelCallback(every_epoch=True, fname=f\"vitb-{patch_sizes}/{patch_strides}_size{size}_lr{lr}_epochs{epochs}\")]\n",
    "if WANDB: cbs += [WandbCallback(log_preds=False,log_model=False)]\n",
    "learn = Learner(dls, model, opt_func=ranger, cbs=cbs,\n",
    "                metrics=[f1macro, f1micro], loss_func=BCEWithLogitsLossFlat())\n",
    "learn.to_fp16();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.05248074531555176, lr_steep=0.43651583790779114)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WANDB: wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
